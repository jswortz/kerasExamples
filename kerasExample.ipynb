{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['currentbalance', 'daysdelinquent', 'previousdueamount',\n",
      "       'previouspaymentamount', 'statmentbalance', 'cardpaymentrestricted',\n",
      "       'bankpaymentrestricted', 'existingrecurringpaymentonaccount',\n",
      "       'balancedue', 'pastduebalance', 'pendingpaymentamount',\n",
      "       'projectedbalancedue', 'recentpaymentamount', 'statementbalance',\n",
      "       'outageflag', 'install', 'broadcastfunction', 'boradcaststatus',\n",
      "       'cancelledappt', 'anilearning', 'accountstatus', 'nonpaydiscoii',\n",
      "       'delinq_days', 'billing', 'due_diff', 'prev_pay_diff', 'bal_due_diff',\n",
      "       'past_due_diff', 'rec_pay_diff', 'promise_diff'],\n",
      "      dtype='object')\n",
      "currentbalance                       float64\n",
      "daysdelinquent                         int64\n",
      "previousdueamount                    float64\n",
      "previouspaymentamount                float64\n",
      "statmentbalance                      float64\n",
      "cardpaymentrestricted                   bool\n",
      "bankpaymentrestricted                   bool\n",
      "existingrecurringpaymentonaccount       bool\n",
      "balancedue                           float64\n",
      "pastduebalance                       float64\n",
      "pendingpaymentamount                 float64\n",
      "projectedbalancedue                  float64\n",
      "recentpaymentamount                  float64\n",
      "statementbalance                     float64\n",
      "outageflag                            object\n",
      "install                               object\n",
      "broadcastfunction                     object\n",
      "boradcaststatus                       object\n",
      "cancelledappt                         object\n",
      "anilearning                           object\n",
      "accountstatus                         object\n",
      "nonpaydiscoii                          int64\n",
      "delinq_days                            int64\n",
      "billing                                int64\n",
      "due_diff                               int64\n",
      "prev_pay_diff                          int64\n",
      "bal_due_diff                           int64\n",
      "past_due_diff                          int64\n",
      "rec_pay_diff                           int64\n",
      "promise_diff                           int64\n",
      "dtype: object\n",
      "['currentbalance', 'daysdelinquent', 'previousdueamount', 'previouspaymentamount', 'statmentbalance', 'balancedue', 'pastduebalance', 'pendingpaymentamount', 'projectedbalancedue', 'recentpaymentamount', 'statementbalance', 'nonpaydiscoii', 'delinq_days', 'due_diff', 'prev_pay_diff', 'bal_due_diff', 'past_due_diff', 'rec_pay_diff', 'promise_diff']\n",
      "['cardpaymentrestricted', 'bankpaymentrestricted', 'existingrecurringpaymentonaccount', 'outageflag', 'install', 'broadcastfunction', 'boradcaststatus', 'cancelledappt', 'anilearning', 'accountstatus']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currentbalance</th>\n",
       "      <th>daysdelinquent</th>\n",
       "      <th>previousdueamount</th>\n",
       "      <th>previouspaymentamount</th>\n",
       "      <th>statmentbalance</th>\n",
       "      <th>cardpaymentrestricted</th>\n",
       "      <th>bankpaymentrestricted</th>\n",
       "      <th>existingrecurringpaymentonaccount</th>\n",
       "      <th>balancedue</th>\n",
       "      <th>pastduebalance</th>\n",
       "      <th>...</th>\n",
       "      <th>accountstatus</th>\n",
       "      <th>nonpaydiscoii</th>\n",
       "      <th>delinq_days</th>\n",
       "      <th>billing</th>\n",
       "      <th>due_diff</th>\n",
       "      <th>prev_pay_diff</th>\n",
       "      <th>bal_due_diff</th>\n",
       "      <th>past_due_diff</th>\n",
       "      <th>rec_pay_diff</th>\n",
       "      <th>promise_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>160.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>160.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-31</td>\n",
       "      <td>7</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.84</td>\n",
       "      <td>389.46</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>389.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>556.25</td>\n",
       "      <td>52</td>\n",
       "      <td>279.25</td>\n",
       "      <td>285.00</td>\n",
       "      <td>556.25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>556.25</td>\n",
       "      <td>279.25</td>\n",
       "      <td>...</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-31</td>\n",
       "      <td>6</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.99</td>\n",
       "      <td>19.99</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>-32</td>\n",
       "      <td>-2</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>414.32</td>\n",
       "      <td>52</td>\n",
       "      <td>200.52</td>\n",
       "      <td>447.12</td>\n",
       "      <td>414.32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>414.32</td>\n",
       "      <td>200.52</td>\n",
       "      <td>...</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-62</td>\n",
       "      <td>6</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   currentbalance  daysdelinquent  previousdueamount  previouspaymentamount  \\\n",
       "0          203.50               0               0.00                 160.04   \n",
       "1          139.62               0               0.00                 249.84   \n",
       "2          556.25              52             279.25                 285.00   \n",
       "3           88.58               0               0.00                  19.99   \n",
       "4          414.32              52             200.52                 447.12   \n",
       "\n",
       "   statmentbalance  cardpaymentrestricted  bankpaymentrestricted  \\\n",
       "0             0.00                  False                  False   \n",
       "1           389.46                  False                  False   \n",
       "2           556.25                  False                  False   \n",
       "3            19.99                  False                  False   \n",
       "4           414.32                  False                  False   \n",
       "\n",
       "   existingrecurringpaymentonaccount  balancedue  pastduebalance     ...       \\\n",
       "0                              False      160.08            0.00     ...        \n",
       "1                              False      389.46            0.00     ...        \n",
       "2                              False      556.25          279.25     ...        \n",
       "3                               True       19.99            0.00     ...        \n",
       "4                              False      414.32          200.52     ...        \n",
       "\n",
       "   accountstatus  nonpaydiscoii  delinq_days  billing due_diff prev_pay_diff  \\\n",
       "0         ACTIVE              0            0        1        7           -31   \n",
       "1         ACTIVE              0            0        0       13            -1   \n",
       "2         ACTIVE             52           30        0        6           -31   \n",
       "3         ACTIVE              0            0        0       -3           -32   \n",
       "4         ACTIVE             52           30        0        6           -62   \n",
       "\n",
       "  bal_due_diff past_due_diff rec_pay_diff promise_diff  \n",
       "0            7         -9999        -9999        -9999  \n",
       "1           13         -9999        -9999        -9999  \n",
       "2            6         -9999        -9999        -9999  \n",
       "3           -2         -9999        -9999        -9999  \n",
       "4            6         -9999        -9999        -9999  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "inputData = pd.read_csv(\"https://s3-us-west-2.amazonaws.com/jeremystorage/cableCo100k.csv\", delimiter = \",\")\n",
    "inputData = inputData.drop(['billingpin'],axis=1)\n",
    "print(inputData.columns)\n",
    "print(inputData.dtypes)\n",
    "# x = [print(x, inputData[x].unique()) for x in inputData.columns]\n",
    "\n",
    "numericCols = [x for x in inputData.columns if inputData[x].dtypes in [\"float64\", \"int64\"]]\n",
    "labelCol = \"billing\"\n",
    "numericCols.remove(labelCol)\n",
    "catCols = [x for x in inputData.columns if x not in numericCols]\n",
    "catCols.remove(labelCol)\n",
    "\n",
    "print(numericCols)\n",
    "print(catCols)\n",
    "\n",
    "inputData.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############PIPELINE EXAMPLE\n",
    "#############DOESNT WORK FOR MIXED TYPES :(\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler, LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing   \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_text_cols(x):\n",
    "    return np.array(x[catCols].astype(str))\n",
    "\n",
    "def get_num_cols(x):\n",
    "    return np.array(x[numericCols])\n",
    "\n",
    "get_text_cols(inputData)\n",
    "preprocessor = make_union(\n",
    "    make_pipeline(\n",
    "        FunctionTransformer(get_num_cols, validate=False),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "    ,\n",
    "    make_pipeline(\n",
    "        FunctionTransformer(get_text_cols, validate=False), LabelEncoder(), MultiLabelBinarizer()\n",
    "       )\n",
    ")\n",
    "\n",
    "# preprocessor.fit(inputData)\n",
    "\n",
    "# newData = preprocessor.transform(inputData)\n",
    "\n",
    "# newData\n",
    "\n",
    "# binar = LabelEncoder()\n",
    "\n",
    "# catData = get_text_cols(inputData)\n",
    "\n",
    "# pl = Pipeline()\n",
    "# mlb = MultiLabelBinarizer().fit_transform(catData)\n",
    "# mlb.shape\n",
    "# len(catCols)\n",
    "# cc.apply(OneHotEncoder().fit_transform)\n",
    "\n",
    "# get_text_cols(np.array(inputData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"pendingpaymentamount\": 0.0, \"anilearning\": \"NOT_FOUND\", \"broadcastfunction\": \"Array\", \"currentbalance\": 203.5, \"install\": \"false\", \"previousdueamount\": 0.0, \"projectedbalancedue\": 160.08, \"previouspaymentamount\": 160.04, \"daysdelinquent\": 0, \"existingrecurringpaymentonaccount\": false, \"cancelledappt\": \"false\", \"statmentbalance\": 0.0, \"outageflag\": \"false\", \"recentpaymentamount\": 160.04, \"rec_pay_diff\": -9999, \"accountstatus\": \"ACTIVE\", \"nonpaydiscoii\": 0, \"promise_diff\": -9999, \"statementbalance\": 0.0, \"balancedue\": 160.08, \"pastduebalance\": 0.0, \"delinq_days\": 0, \"cardpaymentrestricted\": false, \"boradcaststatus\": \"NOT_FOUND\", \"bankpaymentrestricted\": false, \"prev_pay_diff\": -31, \"past_due_diff\": -9999, \"due_diff\": 7, \"bal_due_diff\": 7, \"billing\": 1}'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "len(inputData)\n",
    "# json.loads(str(inputData[:1].to_dict(orient='records')[0]))\n",
    "json.dumps(json.loads(inputData.to_json(orient='records'))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################# Create balanced dataset... just for example\n",
    "import numpy as np\n",
    "\n",
    "def balanced_subsample(y, size=None):\n",
    "\n",
    "    subsample = []\n",
    "\n",
    "    if size is None:\n",
    "        n_smp = y.value_counts().min()\n",
    "    else:\n",
    "        n_smp = int(size / len(y.value_counts().index))\n",
    "\n",
    "    for label in y.value_counts().index:\n",
    "        samples = y[y == label].index.values\n",
    "        index_range = range(samples.shape[0])\n",
    "        indexes = np.random.choice(index_range, size=n_smp, replace=False)\n",
    "        subsample += samples[indexes].tolist()\n",
    "\n",
    "    return subsample\n",
    "\n",
    "ind = balanced_subsample(inputData['billing'])\n",
    "\n",
    "inputData = inputData.loc[inputData.index.isin(ind)]\n",
    "\n",
    "len(np.array(inputData)[0])\n",
    "# inputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cardpaymentrestricted', array([[ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]]), LabelEncoder(), OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "        handle_unknown='error', n_values='auto', sparse=False))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "inputData = inputData.dropna()\n",
    "\n",
    "def catTransform(column):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labeled = le.fit_transform(inputData[column].apply(str))\n",
    "    labeled = [[x] for x in labeled]\n",
    "    enc = OneHotEncoder(sparse = False)\n",
    "    enc.fit_transform(labeled)\n",
    "    oHot = enc.transform(labeled)\n",
    "    return(column, oHot, le, enc)\n",
    "\n",
    "catTransform(\"anilearning\")\n",
    "\n",
    "catTransformed = [catTransform(x) for x in catCols]\n",
    "catTransformed[0]\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\jswortz\\AppData\\Local\\Continuum\\Anaconda3\\envs\\threeFive\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def largeFilter(x, value):\n",
    "    if x > value:\n",
    "        value\n",
    "    else:\n",
    "        x\n",
    "        \n",
    "import numpy as np\n",
    "def contTransform(column):\n",
    "    try:\n",
    "        ss = preprocessing.StandardScaler()\n",
    "        ssfit = ss.fit_transform(inputData[column])\n",
    "        return(column, ssfit, ss)\n",
    "    except:\n",
    "        print(column, inputData[column])\n",
    "\n",
    "contTransform(\"nonpaydiscoii\")\n",
    "\n",
    "contTransformed = [contTransform(x) for x in numericCols]\n",
    "\n",
    "\n",
    "\n",
    "dataFlat = []\n",
    "\n",
    "for x in contTransformed:\n",
    "    dataFlat.append(x[1])\n",
    "\n",
    "    \n",
    "    \n",
    "def flat2gen(alist):\n",
    "    for item in alist:\n",
    "        if isinstance(item, list):\n",
    "            for subitem in item: yield subitem\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "    \n",
    "for x in catTransformed:\n",
    "    l = x[1].tolist()\n",
    "    dataFlat.append(l)\n",
    "\n",
    "dataFlat = zip(*dataFlat)\n",
    "\n",
    "\n",
    "dataFlat = [list(x) for x in dataFlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE STUFF HERE\n",
    "import pickle\n",
    "####TRANSFORM FUNCTION\n",
    "###TRANSFORM FUNCTION\n",
    "\n",
    "def catScore(labelEnc, oneHot, column, data=inputData):\n",
    "    labeled = labelEnc.transform(data[column].apply(str))\n",
    "    labeled = [[x] for x in labeled]\n",
    "    return oneHot.transform(labeled)\n",
    "\n",
    "def contScore(stScale, column, data=inputData):\n",
    "    return stScale.fit(data[column])\n",
    "\n",
    "saveConts = [[x[0], x[2]] for x in contTransformed]\n",
    "\n",
    "saveCats = [[x[0], x[2], x[3]] for x in catTransformed]\n",
    "\n",
    "saveObj = [saveConts, saveCats]\n",
    "\n",
    "with open('kerasTransformersNew.pkl', 'wb') as output:\n",
    "    pickle.dump(saveObj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26450\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "for line in dataFlat:\n",
    "    test = [x for x in flat2gen(line)]\n",
    "    df.append(test)\n",
    "\n",
    "    \n",
    "print(len(df))\n",
    "print(len(df[0]))\n",
    "\n",
    "dataFlat = df\n",
    "# inputData[10]\n",
    "# [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(inputData[labelCol])\n",
    "y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "xFlatTrain, xFlatTest, yTrain, yTest = train_test_split(dataFlat, y, test_size = 0.2, random_state = 111)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(probability=True)\n",
    "\n",
    "model = clf.fit(xFlatTrain, yTrain) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1932  703]\n",
      " [ 881 1774]]\n",
      "accuracy\n",
      "0.70056710775\n",
      "recall\n",
      "0.668173258004\n",
      "precision\n",
      "0.716188938232\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "yPred = model.predict(xFlatTest)\n",
    "\n",
    "print(confusion_matrix(yTest, yPred))\n",
    "print(\"accuracy\")\n",
    "print(sklearn.metrics.accuracy_score(yTest, yPred))\n",
    "print(\"recall\")\n",
    "print(sklearn.metrics.recall_score(yTest, yPred))\n",
    "print(\"precision\")\n",
    "print(sklearn.metrics.precision_score(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sklearnSVCVanilla.pkl', 'wb') as output:\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26450, 47)\n",
      "[<tf.Tensor 'dense_7/Sigmoid:0' shape=(?, 1) dtype=float32>]\n",
      "<bound method Container.summary of <keras.models.Sequential object at 0x00000000231DF1D0>>\n",
      "[{'config': {'trainable': True, 'units': 47, 'kernel_initializer': {'config': {'seed': None, 'mode': 'fan_avg', 'distribution': 'uniform', 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'use_bias': True, 'kernel_constraint': None, 'activity_regularizer': None, 'activation': 'linear', 'kernel_regularizer': {'config': {'l1': 0.0, 'l2': 0.10000000149011612}, 'class_name': 'L1L2'}, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'batch_input_shape': (None, 47), 'bias_regularizer': None, 'name': 'dense_5', 'bias_constraint': None, 'dtype': 'float32'}, 'class_name': 'Dense'}, {'config': {'units': 20, 'kernel_initializer': {'config': {'seed': None, 'mode': 'fan_avg', 'distribution': 'uniform', 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'bias_constraint': None, 'name': 'dense_6', 'kernel_constraint': None, 'activity_regularizer': None, 'activation': 'linear', 'kernel_regularizer': {'config': {'l1': 0.0, 'l2': 0.10000000149011612}, 'class_name': 'L1L2'}, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'bias_regularizer': None, 'use_bias': True, 'trainable': True}, 'class_name': 'Dense'}, {'config': {'units': 1, 'kernel_initializer': {'config': {'seed': None, 'mode': 'fan_avg', 'distribution': 'uniform', 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'bias_constraint': None, 'name': 'dense_7', 'kernel_constraint': None, 'activity_regularizer': None, 'activation': 'sigmoid', 'kernel_regularizer': None, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}, 'bias_regularizer': None, 'use_bias': True, 'trainable': True}, 'class_name': 'Dense'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-0.01744074, -0.2067862 , -0.13004673, ...,  0.22604251,\n",
       "          0.12052622, -0.23404127],\n",
       "        [-0.14982176, -0.11989357, -0.20355445, ..., -0.12788559,\n",
       "          0.11667162,  0.20737103],\n",
       "        [-0.24255247,  0.2109457 , -0.18335891, ...,  0.02039251,\n",
       "         -0.11694607, -0.10833493],\n",
       "        ..., \n",
       "        [-0.21827538, -0.02218759, -0.08207531, ..., -0.16810833,\n",
       "          0.10599616, -0.11166945],\n",
       "        [ 0.02965635, -0.15594795,  0.18923849, ..., -0.13912186,\n",
       "         -0.00571917, -0.05503304],\n",
       "        [ 0.05225524, -0.06998493, -0.0176235 , ..., -0.20841798,\n",
       "         -0.04308   , -0.21003084]], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([[  7.50052631e-02,   2.19510555e-01,   9.15490389e-02,\n",
       "          -1.57935739e-01,  -2.59584785e-01,   1.24340236e-01,\n",
       "          -1.96230769e-01,  -2.19693929e-01,   5.44368625e-02,\n",
       "           2.58204579e-01,   1.25626355e-01,   1.77948624e-01,\n",
       "          -2.51381814e-01,   1.90348744e-01,  -1.15453988e-01,\n",
       "          -1.50814265e-01,  -1.02594897e-01,   2.35349119e-01,\n",
       "           2.91877389e-01,  -2.87102163e-02],\n",
       "        [  4.43621278e-02,  -1.99506462e-01,  -2.01946408e-01,\n",
       "           1.99701726e-01,  -1.27250060e-01,  -2.81054378e-01,\n",
       "          -8.35095197e-02,   8.32535326e-02,   1.64645821e-01,\n",
       "           2.74500906e-01,  -9.97044593e-02,  -2.84266651e-01,\n",
       "          -7.35803843e-02,   2.31754839e-01,   5.40810525e-02,\n",
       "           2.93648839e-02,   1.76334381e-02,  -1.91218168e-01,\n",
       "          -2.92521179e-01,  -2.20086992e-01],\n",
       "        [ -2.10738838e-01,   2.61783600e-04,   2.35826850e-01,\n",
       "          -3.13639641e-04,   8.98571014e-02,   2.55066991e-01,\n",
       "          -9.38930809e-02,  -2.78488278e-01,  -3.87688279e-02,\n",
       "          -3.24244797e-02,  -1.56165972e-01,   5.62445223e-02,\n",
       "          -2.12852001e-01,  -2.56404817e-01,   2.02871323e-01,\n",
       "           2.75262833e-01,  -1.93372726e-01,  -2.06982166e-01,\n",
       "          -2.20899999e-01,  -2.44139344e-01],\n",
       "        [  2.68874645e-01,   1.19717479e-01,   8.55761766e-02,\n",
       "           2.55443752e-01,   1.85997814e-01,  -2.89472431e-01,\n",
       "           2.53612638e-01,  -2.02715963e-01,  -3.90281081e-02,\n",
       "           1.66041225e-01,   1.38558149e-01,   2.54274368e-01,\n",
       "           6.72715604e-02,   7.52633214e-02,  -9.05607194e-02,\n",
       "           1.58829570e-01,   1.68820649e-01,  -1.07364535e-01,\n",
       "           5.85611761e-02,  -1.68932810e-01],\n",
       "        [ -7.21398145e-02,  -2.06460118e-01,   4.28192317e-02,\n",
       "           1.47398710e-02,   3.07475328e-02,   7.25039840e-02,\n",
       "           2.71938443e-02,  -1.93852678e-01,  -2.73465276e-01,\n",
       "          -1.39108077e-01,  -1.87419772e-01,  -4.66099977e-02,\n",
       "          -6.07638955e-02,   2.80472934e-01,   1.82178169e-01,\n",
       "          -4.63141799e-02,   1.83277220e-01,   1.24396980e-02,\n",
       "          -1.11437544e-01,   2.58877099e-01],\n",
       "        [ -2.73985624e-01,   2.05018342e-01,  -1.11933485e-01,\n",
       "           1.53532684e-01,  -1.57069862e-01,   3.44319940e-02,\n",
       "           1.09937549e-01,   2.40951538e-01,   4.26303148e-02,\n",
       "          -1.57989323e-01,  -1.74789146e-01,   2.36270666e-01,\n",
       "          -8.56059939e-02,  -2.04610303e-01,   5.08477390e-02,\n",
       "           5.78960180e-02,   2.27980733e-01,  -1.38596952e-01,\n",
       "          -7.91439265e-02,  -1.10079601e-01],\n",
       "        [  6.34618402e-02,   3.68277729e-02,  -2.67590582e-02,\n",
       "          -2.05391273e-01,   2.93718696e-01,   1.78830326e-01,\n",
       "          -1.16276696e-01,  -1.61292642e-01,  -2.69514829e-01,\n",
       "           1.07855052e-01,  -1.31970048e-02,  -1.82291895e-01,\n",
       "           2.79597878e-01,  -1.94514722e-01,  -8.73204023e-02,\n",
       "           2.38269806e-01,  -2.72280425e-01,  -2.02628404e-01,\n",
       "           3.00677419e-02,   7.12777972e-02],\n",
       "        [ -1.65290803e-01,   2.35942900e-01,  -2.35596567e-01,\n",
       "          -9.37259793e-02,   2.95305848e-01,  -1.04359984e-02,\n",
       "           2.50182033e-01,   6.89447224e-02,  -2.96051085e-01,\n",
       "           2.95933008e-01,   1.70386523e-01,  -1.80408895e-01,\n",
       "           2.01355577e-01,  -2.04083323e-01,   2.71982253e-02,\n",
       "           1.82955146e-01,   2.20124245e-01,  -2.50421911e-01,\n",
       "          -2.02072620e-01,   1.85265154e-01],\n",
       "        [ -2.70339698e-01,   2.17079163e-01,   2.65021861e-01,\n",
       "          -1.09718651e-01,  -1.32562950e-01,  -5.49173951e-02,\n",
       "           1.59129322e-01,  -1.77199244e-02,  -2.27543801e-01,\n",
       "           2.15049684e-01,   1.68712407e-01,  -2.52684981e-01,\n",
       "           5.23118675e-02,  -2.59632856e-01,  -1.18549243e-01,\n",
       "          -6.07952178e-02,  -2.62076735e-01,  -2.32126653e-01,\n",
       "          -1.72533631e-01,  -1.09818250e-01],\n",
       "        [  2.86917627e-01,  -1.44570589e-01,  -4.15259898e-02,\n",
       "          -1.40765011e-02,  -2.26093441e-01,  -2.20698416e-02,\n",
       "          -4.37043011e-02,  -1.01703480e-01,   2.87637353e-01,\n",
       "          -2.51242280e-01,   5.04869223e-03,  -6.33172691e-02,\n",
       "           1.00514472e-01,  -7.16228336e-02,   5.64387441e-02,\n",
       "          -1.83579922e-01,  -1.79332122e-01,   3.47006023e-02,\n",
       "          -2.07997665e-01,   1.95726335e-01],\n",
       "        [ -1.59039199e-01,  -1.59917206e-01,  -1.30933657e-01,\n",
       "           9.77045894e-02,  -1.21092200e-02,   1.38518751e-01,\n",
       "          -4.44512963e-02,   2.87721872e-01,  -1.58007234e-01,\n",
       "           2.01172590e-01,   5.80725372e-02,  -1.98720574e-01,\n",
       "          -5.71918786e-02,  -5.15927523e-02,  -5.05323857e-02,\n",
       "           2.17828453e-01,   1.65416092e-01,  -2.29750425e-01,\n",
       "          -1.04661405e-01,  -2.74872810e-01],\n",
       "        [ -5.27404547e-02,   2.87682295e-01,   4.45948541e-02,\n",
       "          -7.69621134e-02,   2.01217830e-01,  -2.62562960e-01,\n",
       "           2.38650739e-01,   1.87695831e-01,   2.26595342e-01,\n",
       "          -2.88431555e-01,  -2.18828410e-01,  -1.79921299e-01,\n",
       "          -2.32479677e-01,  -2.31534824e-01,   1.17473394e-01,\n",
       "           1.22546136e-01,   2.90383637e-01,   1.71173602e-01,\n",
       "          -2.26823688e-01,  -1.56303674e-01],\n",
       "        [  1.62135184e-01,   9.36304331e-02,  -1.36800274e-01,\n",
       "           1.81454420e-02,   2.64057398e-01,   7.58926868e-02,\n",
       "          -2.04572052e-01,  -2.09617108e-01,  -1.61833733e-01,\n",
       "           2.98933685e-02,  -3.73481810e-02,   3.61667871e-02,\n",
       "          -1.74443752e-01,  -2.65763104e-01,  -2.22603559e-01,\n",
       "          -2.69686013e-01,  -1.33899212e-01,  -2.95593470e-01,\n",
       "          -1.71900362e-01,  -1.91806719e-01],\n",
       "        [ -1.89198822e-01,  -1.50811628e-01,   1.07903779e-01,\n",
       "           1.44523561e-01,  -2.21275985e-01,   1.83284551e-01,\n",
       "          -2.21482188e-01,  -6.36070222e-02,   4.82003391e-02,\n",
       "           2.42772818e-01,   6.65384829e-02,  -3.08956504e-02,\n",
       "          -2.88620114e-01,  -2.26385742e-01,   4.83829081e-02,\n",
       "           2.71235824e-01,   5.96370399e-02,   2.35991240e-01,\n",
       "          -1.93585768e-01,  -5.22106886e-02],\n",
       "        [ -2.79452324e-01,   2.50207305e-01,   7.29568005e-02,\n",
       "           1.74542427e-01,  -1.17005438e-01,  -2.96995580e-01,\n",
       "          -8.41543078e-03,  -1.25228226e-01,   1.03423089e-01,\n",
       "          -1.58493027e-01,   3.56876850e-02,  -2.19644070e-01,\n",
       "           9.03898478e-02,  -5.35108596e-02,  -1.60100415e-01,\n",
       "          -7.83424675e-02,   8.94203782e-03,   7.19922781e-02,\n",
       "          -2.72615671e-01,  -2.47739673e-01],\n",
       "        [ -2.31850952e-01,  -5.81400245e-02,  -1.06536910e-01,\n",
       "          -1.91476807e-01,  -1.99396014e-02,  -1.15879849e-01,\n",
       "           1.18003666e-01,  -2.51010448e-01,  -5.25027215e-02,\n",
       "           2.40508497e-01,  -1.74553201e-01,  -2.63668925e-01,\n",
       "          -1.83998242e-01,  -1.10886246e-01,   2.98694134e-01,\n",
       "           8.99434090e-02,  -7.84407854e-02,   6.21370375e-02,\n",
       "           5.41667640e-02,   2.03452289e-01],\n",
       "        [ -1.19497880e-01,   2.50868678e-01,   2.33956218e-01,\n",
       "           8.85915458e-02,  -1.82698503e-01,  -1.81923538e-01,\n",
       "          -4.56316173e-02,  -2.53467530e-01,   2.13362038e-01,\n",
       "          -2.82040536e-01,   1.75860643e-01,  -5.77868521e-02,\n",
       "          -5.10835499e-02,   2.72081077e-01,  -2.59512007e-01,\n",
       "           2.47997820e-01,  -2.98865676e-01,  -3.90989482e-02,\n",
       "           1.83802336e-01,   2.03985870e-01],\n",
       "        [  4.41280901e-02,  -1.71105623e-01,  -2.51060486e-01,\n",
       "           2.64514863e-01,   1.35513663e-01,   1.36591017e-01,\n",
       "           4.45767939e-02,  -2.32208550e-01,   6.80509508e-02,\n",
       "           2.03450620e-02,  -1.52728513e-01,   1.04525626e-01,\n",
       "           4.57911491e-02,  -1.52781323e-01,   1.89550221e-02,\n",
       "           2.61433899e-01,   2.86554694e-01,   1.74458444e-01,\n",
       "           1.50182545e-01,   7.70593584e-02],\n",
       "        [ -5.50763607e-02,  -1.17321000e-01,  -1.35903299e-01,\n",
       "          -3.04618478e-02,  -2.24751890e-01,   1.91225082e-01,\n",
       "          -4.97061163e-02,  -3.31034958e-02,  -2.58641362e-01,\n",
       "          -6.20352328e-02,  -1.55769557e-01,  -1.66892409e-01,\n",
       "           2.29000866e-01,   1.36503458e-01,   1.52445257e-01,\n",
       "          -7.29858875e-05,   2.40369320e-01,  -1.33236945e-02,\n",
       "           2.81971157e-01,   1.80526406e-01],\n",
       "        [ -6.79472983e-02,  -5.60751557e-02,   8.88818502e-02,\n",
       "           1.65264904e-02,  -1.10082015e-01,  -1.42471045e-01,\n",
       "          -2.90994048e-02,   7.46603012e-02,   2.51516402e-01,\n",
       "           1.26652092e-01,  -2.66954124e-01,   2.93647468e-01,\n",
       "           1.37903988e-02,   2.29464889e-01,  -1.12176135e-01,\n",
       "          -5.68988621e-02,   7.60283172e-02,   9.45267677e-02,\n",
       "           6.68352842e-03,  -2.38474607e-02],\n",
       "        [ -7.58277625e-02,  -7.90988952e-02,  -6.09421879e-02,\n",
       "           1.16038322e-01,   1.47586733e-01,   1.66431993e-01,\n",
       "          -1.71335712e-01,  -1.02363944e-01,   2.01421022e-01,\n",
       "          -5.19327968e-02,  -8.78353864e-02,  -1.64074466e-01,\n",
       "           5.09621799e-02,   1.40023172e-01,   7.02520311e-02,\n",
       "          -1.57793760e-01,  -1.54970258e-01,  -2.86242604e-01,\n",
       "           7.12481141e-03,   2.79841602e-01],\n",
       "        [  3.60955298e-02,   7.47942328e-02,  -1.32432893e-01,\n",
       "          -1.33056030e-01,  -9.15684253e-02,  -1.24851719e-01,\n",
       "          -2.86060750e-01,  -2.64881313e-01,  -1.97150707e-01,\n",
       "           2.84655929e-01,  -2.21755952e-01,  -1.16948217e-01,\n",
       "           8.62019658e-02,   2.93538868e-01,   1.31656259e-01,\n",
       "          -1.10696539e-01,   5.22711873e-03,  -2.52434909e-02,\n",
       "          -2.26658508e-01,  -2.46382654e-01],\n",
       "        [ -9.67422128e-03,   2.68361449e-01,   1.48860812e-02,\n",
       "          -1.56183451e-01,   1.61735713e-01,   1.69009864e-02,\n",
       "          -2.80190200e-01,  -1.83548748e-01,   2.16541529e-01,\n",
       "           2.01459527e-02,   2.16482639e-01,  -1.65201902e-01,\n",
       "           2.36079574e-01,  -4.77406383e-03,   1.03359312e-01,\n",
       "           1.83789849e-01,  -1.49269804e-01,  -2.25539148e-01,\n",
       "           1.74041837e-01,   1.80250943e-01],\n",
       "        [  8.07037055e-02,  -4.98134196e-02,   1.80383295e-01,\n",
       "          -2.23841503e-01,   7.81495571e-02,  -8.47620964e-02,\n",
       "           3.45946550e-02,   1.13056123e-01,   2.96050549e-01,\n",
       "          -1.73685536e-01,   1.91656947e-02,  -4.29450274e-02,\n",
       "           1.40664309e-01,   1.25570536e-01,  -2.36912131e-01,\n",
       "          -2.31349394e-01,  -2.19030544e-01,  -2.04572916e-01,\n",
       "          -8.89782310e-02,   1.84669554e-01],\n",
       "        [  1.49200380e-01,   1.50876045e-01,  -3.75911891e-02,\n",
       "           2.75637090e-01,  -2.52583385e-01,   2.28514850e-01,\n",
       "           2.28291750e-02,  -2.33463645e-02,   2.95354545e-01,\n",
       "           2.48300731e-01,   1.15176141e-01,   1.86064810e-01,\n",
       "           1.60273284e-01,   1.45101547e-02,   9.96237099e-02,\n",
       "          -2.58696288e-01,   2.60582864e-02,   2.71542668e-02,\n",
       "          -1.28992021e-01,   1.95230246e-02],\n",
       "        [  2.14736342e-01,  -1.57984823e-01,   5.99724054e-03,\n",
       "           5.27141988e-02,  -1.35634750e-01,   2.61766136e-01,\n",
       "           2.33558655e-01,   1.68459058e-01,  -1.01538315e-01,\n",
       "          -5.40614426e-02,   2.01223552e-01,   1.44473404e-01,\n",
       "          -1.68331057e-01,  -1.78282380e-01,   1.57136291e-01,\n",
       "           2.80123115e-01,   9.15020108e-02,   2.29447365e-01,\n",
       "           1.50235862e-01,  -1.54038742e-01],\n",
       "        [  2.24544406e-01,  -2.61136025e-01,  -2.25979358e-01,\n",
       "          -1.52685642e-01,  -1.86601713e-01,   6.56245053e-02,\n",
       "           1.24404520e-01,  -3.24410200e-03,  -2.36228049e-01,\n",
       "          -1.19698018e-01,   2.17967272e-01,  -2.05312356e-01,\n",
       "          -1.72221571e-01,  -1.83446735e-01,  -2.13799268e-01,\n",
       "          -2.30688289e-01,  -2.30635345e-01,  -9.34901685e-02,\n",
       "          -2.71868795e-01,   2.85518169e-01],\n",
       "        [ -9.28506106e-02,  -1.46997392e-01,   2.42387891e-01,\n",
       "           2.70738602e-02,   2.18801260e-01,  -2.56373346e-01,\n",
       "          -1.17847681e-01,   2.64968455e-01,   1.41898751e-01,\n",
       "          -2.86665618e-01,   2.23091900e-01,  -8.40733796e-02,\n",
       "          -9.70117450e-02,   1.83953285e-01,   2.34035850e-02,\n",
       "          -8.34558755e-02,   3.22838426e-02,  -1.14367858e-01,\n",
       "           5.18291295e-02,   7.31129944e-02],\n",
       "        [ -2.41404891e-01,  -2.69091904e-01,   1.53904855e-02,\n",
       "          -2.11535141e-01,  -4.69535291e-02,   6.61236644e-02,\n",
       "           2.42166758e-01,  -2.34360546e-01,   5.19851744e-02,\n",
       "          -8.88633728e-02,  -2.61681825e-01,  -2.83850044e-01,\n",
       "           1.61113828e-01,   2.71513045e-01,   1.57794118e-01,\n",
       "          -2.50145227e-01,  -8.63659233e-02,  -2.91860849e-01,\n",
       "          -1.81079060e-01,  -8.02033544e-02],\n",
       "        [  1.74203217e-02,   7.23133385e-02,   1.03224814e-01,\n",
       "           1.04772121e-01,  -2.30405107e-01,  -1.31543323e-01,\n",
       "           6.34898543e-02,  -1.36492774e-01,  -1.55629650e-01,\n",
       "           4.39694226e-02,  -1.08374253e-01,  -7.12404847e-02,\n",
       "          -2.02800363e-01,  -1.70787871e-02,  -2.47814670e-01,\n",
       "           2.14425564e-01,  -2.21313655e-01,  -8.78786296e-02,\n",
       "          -8.39398205e-02,  -2.13377044e-01],\n",
       "        [  2.58646131e-01,  -6.47158325e-02,   1.13848597e-01,\n",
       "           7.69495964e-03,  -1.36495531e-02,  -1.79090396e-01,\n",
       "           3.73846889e-02,   2.41316080e-01,  -4.47331369e-02,\n",
       "          -3.33457887e-02,   3.70950401e-02,   2.86875963e-02,\n",
       "          -1.18566453e-02,   2.04510391e-01,  -2.85209119e-02,\n",
       "          -2.30478674e-01,  -1.37163579e-01,  -2.08704785e-01,\n",
       "           2.02854931e-01,   1.74266309e-01],\n",
       "        [  2.46618867e-01,   1.85521215e-01,   1.04198724e-01,\n",
       "           1.70477629e-02,   2.02268124e-01,   3.64780128e-02,\n",
       "          -3.60171199e-02,   2.56329894e-01,  -9.66358185e-02,\n",
       "           1.93442941e-01,   1.14151597e-01,   1.57841623e-01,\n",
       "          -2.21607402e-01,  -1.33786127e-01,   2.76322961e-02,\n",
       "           1.04621381e-01,   1.52219325e-01,   1.49739265e-01,\n",
       "          -9.03207064e-03,   1.28933728e-01],\n",
       "        [  2.77517021e-01,  -2.44400829e-01,   2.68598258e-01,\n",
       "          -2.72471279e-01,  -9.90367234e-02,   6.88102841e-03,\n",
       "          -2.40524590e-01,   2.18861163e-01,  -1.99236870e-02,\n",
       "          -7.74773806e-02,  -7.90603012e-02,   1.98563755e-01,\n",
       "          -1.70668900e-01,  -2.47976124e-01,   1.47383958e-01,\n",
       "           2.06157029e-01,  -5.60816526e-02,   1.33136660e-01,\n",
       "          -1.34481773e-01,   2.28773236e-01],\n",
       "        [  2.85132706e-01,   6.58729672e-03,  -3.27784419e-02,\n",
       "          -1.59832224e-01,  -2.90057123e-01,   2.01415598e-01,\n",
       "          -2.72179872e-01,  -1.84578940e-01,  -6.59880936e-02,\n",
       "          -4.48904335e-02,  -1.30072832e-02,  -1.45640433e-01,\n",
       "           3.32452655e-02,   1.16154969e-01,  -5.35349697e-02,\n",
       "          -2.00064480e-01,   6.57312572e-02,  -1.87326461e-01,\n",
       "           1.99092567e-01,   2.47301221e-01],\n",
       "        [ -2.39453882e-01,  -2.91950107e-02,  -1.81158617e-01,\n",
       "          -8.58515501e-03,  -4.92477119e-02,  -4.10211980e-02,\n",
       "           2.68727362e-01,   1.47363394e-01,  -2.11213365e-01,\n",
       "          -2.32467413e-01,  -2.30948418e-01,  -1.16288528e-01,\n",
       "           1.41578555e-01,  -2.02360570e-01,  -9.30240750e-03,\n",
       "          -1.42832339e-01,   2.54259884e-01,  -1.59421623e-01,\n",
       "           5.21473289e-02,  -2.47703999e-01],\n",
       "        [  3.06444466e-02,  -2.84017146e-01,   2.41049528e-01,\n",
       "          -1.97893292e-01,  -4.22888398e-02,   1.64237499e-01,\n",
       "           1.65382206e-01,   8.00816417e-02,  -1.95903271e-01,\n",
       "           1.29145563e-01,   1.05295390e-01,   7.17190802e-02,\n",
       "           1.83157921e-01,   1.74267083e-01,   2.79108644e-01,\n",
       "          -1.02823347e-01,   1.10118687e-01,   1.30930394e-01,\n",
       "          -2.38220289e-01,  -2.91722238e-01],\n",
       "        [  2.01082885e-01,  -1.40453473e-01,   2.93268740e-01,\n",
       "          -1.59603357e-02,   9.35764313e-02,   1.07318163e-01,\n",
       "          -1.59574240e-01,  -9.10732150e-02,   1.31094456e-02,\n",
       "           2.72748232e-01,   1.57512873e-01,   9.21462178e-02,\n",
       "          -1.46389008e-01,   2.23085940e-01,   8.20939839e-02,\n",
       "          -1.85798258e-01,  -1.17438659e-01,   2.49585509e-01,\n",
       "          -7.46355504e-02,   1.90653533e-01],\n",
       "        [ -2.61683464e-01,   6.33806288e-02,  -1.67080984e-01,\n",
       "          -2.42695063e-01,   1.52428061e-01,   2.25679815e-01,\n",
       "           1.01332903e-01,  -3.96003127e-02,  -3.90041471e-02,\n",
       "          -2.06545234e-01,  -8.80267471e-02,   1.44056886e-01,\n",
       "          -2.19408333e-01,  -2.18228310e-01,  -3.02722156e-02,\n",
       "           1.19914323e-01,   2.54253149e-01,  -1.28350809e-01,\n",
       "          -2.52299637e-01,   2.34985590e-01],\n",
       "        [ -1.53471962e-01,   1.55883431e-01,   2.28090227e-01,\n",
       "          -2.16894686e-01,   2.78839946e-01,  -1.76049858e-01,\n",
       "          -1.37699038e-01,   7.63260424e-02,   1.66148901e-01,\n",
       "          -2.44099602e-01,   1.76423013e-01,  -6.13083392e-02,\n",
       "           2.49953687e-01,   1.83539331e-01,   1.98130518e-01,\n",
       "          -2.85923839e-01,   2.48345077e-01,  -1.84717178e-02,\n",
       "          -2.63204873e-02,   2.97859609e-01],\n",
       "        [ -2.47545183e-01,   1.00835472e-01,  -2.90718943e-01,\n",
       "          -2.59167254e-02,   1.64987803e-01,   1.75274312e-01,\n",
       "           2.66482234e-01,  -2.47397780e-01,   2.39843607e-01,\n",
       "          -1.89382121e-01,  -7.76199400e-02,   6.11373186e-02,\n",
       "          -2.84746677e-01,   5.27593493e-02,   8.64985585e-02,\n",
       "           3.88447642e-02,  -2.52280146e-01,  -2.75564402e-01,\n",
       "          -9.74222720e-02,  -2.11064219e-02],\n",
       "        [ -2.03252271e-01,  -2.02983588e-01,  -2.73408711e-01,\n",
       "           9.40001607e-02,  -1.80236235e-01,   2.64462233e-02,\n",
       "           2.47070789e-01,  -7.53645748e-02,  -1.06877312e-01,\n",
       "          -9.85310823e-02,   1.55230612e-01,   2.63086140e-01,\n",
       "          -2.47298181e-01,  -2.31971681e-01,   4.44535017e-02,\n",
       "           8.28790367e-02,   2.34351635e-01,   1.13686860e-01,\n",
       "          -2.08345056e-02,  -9.38254595e-03],\n",
       "        [  1.25213444e-01,  -1.39783815e-01,  -1.62765175e-01,\n",
       "           5.63026965e-02,   1.15510285e-01,  -2.84882486e-02,\n",
       "          -1.78641915e-01,   5.71712852e-03,  -1.20722562e-01,\n",
       "           1.30639583e-01,  -1.97838500e-01,   1.82991445e-01,\n",
       "          -4.64417636e-02,  -6.96591288e-02,   1.74284577e-02,\n",
       "           2.46900320e-01,   2.80246377e-01,   2.55899727e-01,\n",
       "           4.70057130e-03,   1.68121427e-01],\n",
       "        [ -4.10037339e-02,  -1.69203281e-01,   1.10213876e-01,\n",
       "          -1.57016709e-01,  -6.43949062e-02,   1.07026070e-01,\n",
       "           1.72201395e-02,  -1.36198252e-01,  -1.35588720e-01,\n",
       "           8.25046599e-02,   6.84555769e-02,  -1.00812986e-01,\n",
       "           2.50457823e-01,   5.81488609e-02,   1.09568238e-02,\n",
       "           5.42632043e-02,   1.12700462e-01,  -1.23046771e-01,\n",
       "          -1.92499861e-01,   2.90653825e-01],\n",
       "        [ -2.77879894e-01,  -9.12356675e-02,   3.02036405e-02,\n",
       "          -1.56094119e-01,   3.34557295e-02,   1.28580421e-01,\n",
       "          -1.20622814e-02,  -2.15594888e-01,   2.78750598e-01,\n",
       "          -2.56996930e-01,  -1.20014369e-01,  -1.05239734e-01,\n",
       "          -1.42084062e-02,  -1.87419355e-01,  -1.25731438e-01,\n",
       "          -4.11781669e-02,  -2.82034397e-01,  -2.64136463e-01,\n",
       "          -2.68418163e-01,   2.44025946e-01],\n",
       "        [  2.44146168e-01,  -4.56704199e-02,   5.24422228e-02,\n",
       "          -1.81442201e-02,  -3.36873233e-02,   1.61660999e-01,\n",
       "           2.18882859e-01,   1.79173857e-01,   6.99338913e-02,\n",
       "          -5.91870397e-02,   1.28239721e-01,   7.90500343e-02,\n",
       "          -1.32063329e-02,   1.69097394e-01,   6.24847412e-03,\n",
       "           8.87621343e-02,  -2.83811808e-01,  -6.00150377e-02,\n",
       "          -1.99771225e-01,   2.41902292e-01],\n",
       "        [  1.83876395e-01,  -1.18645638e-01,  -6.10575676e-02,\n",
       "          -1.55717120e-01,   1.60192668e-01,  -5.14271557e-02,\n",
       "          -2.49380529e-01,   1.85731351e-01,   2.68023014e-02,\n",
       "           1.10617846e-01,   1.40081972e-01,   7.03696907e-02,\n",
       "          -1.47009239e-01,   1.70734257e-01,   2.28957891e-01,\n",
       "           6.13138080e-03,  -1.05896711e-01,   2.95817077e-01,\n",
       "          -2.35848278e-01,   9.73549485e-03],\n",
       "        [  2.61895835e-01,   1.63798362e-01,   1.25667751e-02,\n",
       "           1.57232106e-01,   1.35713875e-01,   2.67498493e-01,\n",
       "          -7.92332441e-02,   2.45762289e-01,  -2.23473340e-01,\n",
       "          -2.17512280e-01,   9.24741030e-02,  -2.71812081e-03,\n",
       "          -3.93324196e-02,  -2.83038527e-01,  -1.91871136e-01,\n",
       "          -4.16736007e-02,  -2.72867084e-03,   2.91381478e-01,\n",
       "           2.61715353e-02,   6.95909262e-02]], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([[-0.48542798],\n",
       "        [ 0.50803834],\n",
       "        [ 0.16292441],\n",
       "        [ 0.17651296],\n",
       "        [ 0.44585478],\n",
       "        [-0.26645443],\n",
       "        [-0.20260847],\n",
       "        [ 0.00229293],\n",
       "        [ 0.41013664],\n",
       "        [-0.27741873],\n",
       "        [-0.51510698],\n",
       "        [-0.03581652],\n",
       "        [-0.20984438],\n",
       "        [ 0.38653982],\n",
       "        [-0.26642728],\n",
       "        [-0.03725547],\n",
       "        [-0.17184213],\n",
       "        [-0.417319  ],\n",
       "        [-0.10941368],\n",
       "        [ 0.32853198]], dtype=float32),\n",
       " array([ 0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "# Import `Sequential` from `keras.models`\n",
    "from keras import regularizers\n",
    "# import h5py as dklfjl\n",
    "\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense, Dropout, Maximum, Flatten\n",
    "\n",
    "# Initialize the constructor\n",
    "model = Sequential()\n",
    "\n",
    "print(np.array(dataFlat).shape)\n",
    "\n",
    "# Add an input layer \n",
    "model.add(Dense(47, activation='linear', input_shape=(47,),  kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "# Add one hidden layer \n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='linear', kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "# model.add(Dense(200, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "# model.add(Dense(200, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "# # Add one hidden layer \n",
    "# model.add(Dense(150, activation='tanh'))\n",
    "# # Add one hidden layer \n",
    "# # model.add(Dropout(0.1))\n",
    "# model.add(Dense(120, activation='relu'))\n",
    "# # Add one hidden layer \n",
    "# # model.add(Dropout(.1))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# model.add(Dense(1000, trainable=True, init='he_normal', activation='relu', \n",
    "#                 W_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256, trainable=True, init='he_normal', activation='linear', \n",
    "#                 W_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Dense(80, activation='tanh'))\n",
    "# model.add(Dense(12, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "\n",
    "tbCB = keras.callbacks.TensorBoard(log_dir='c:\\\\users\\\\Jeremy\\\\logs', histogram_freq=0, write_graph=True, \n",
    "                            write_images=True, embeddings_freq=0, embeddings_layer_names=None, \n",
    "                            embeddings_metadata=None)\n",
    "tbCB.set_model(model)\n",
    "# Model output shape\n",
    "print(model.outputs)\n",
    "\n",
    "# Model summary\n",
    "print(model.summary)\n",
    "\n",
    "# Model config\n",
    "print(model.get_config())\n",
    "\n",
    "# List all weight tensors \n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1.0072092582052738}\n",
      "Have to beat this accuracy\n",
      "Train on 19044 samples, validate on 2116 samples\n",
      "Epoch 1/500\n",
      "19044/19044 [==============================] - 0s - loss: 7.6100 - acc: 0.5898 - val_loss: 6.7572 - val_acc: 0.6342\n",
      "Epoch 2/500\n",
      "19044/19044 [==============================] - 0s - loss: 6.1533 - acc: 0.6405 - val_loss: 5.4732 - val_acc: 0.6503\n",
      "Epoch 3/500\n",
      "19044/19044 [==============================] - 0s - loss: 4.9869 - acc: 0.6557 - val_loss: 4.4393 - val_acc: 0.6531\n",
      "Epoch 4/500\n",
      "19044/19044 [==============================] - 0s - loss: 4.0495 - acc: 0.6550 - val_loss: 3.6106 - val_acc: 0.6607\n",
      "Epoch 5/500\n",
      "19044/19044 [==============================] - 0s - loss: 3.3005 - acc: 0.6594 - val_loss: 2.9507 - val_acc: 0.6612\n",
      "Epoch 6/500\n",
      "19044/19044 [==============================] - 0s - loss: 2.7052 - acc: 0.6624 - val_loss: 2.4284 - val_acc: 0.6682\n",
      "Epoch 7/500\n",
      "19044/19044 [==============================] - 0s - loss: 2.2345 - acc: 0.6690 - val_loss: 2.0164 - val_acc: 0.6716\n",
      "Epoch 8/500\n",
      "19044/19044 [==============================] - 0s - loss: 1.8646 - acc: 0.6730 - val_loss: 1.6932 - val_acc: 0.6772\n",
      "Epoch 9/500\n",
      "19044/19044 [==============================] - 0s - loss: 1.5763 - acc: 0.6725 - val_loss: 1.4422 - val_acc: 0.6749\n",
      "Epoch 10/500\n",
      "19044/19044 [==============================] - 0s - loss: 1.3511 - acc: 0.6723 - val_loss: 1.2472 - val_acc: 0.6772\n",
      "Epoch 11/500\n",
      "19044/19044 [==============================] - 0s - loss: 1.1781 - acc: 0.6738 - val_loss: 1.0984 - val_acc: 0.6777\n",
      "Epoch 12/500\n",
      "19044/19044 [==============================] - 0s - loss: 1.0458 - acc: 0.6762 - val_loss: 0.9851 - val_acc: 0.6782\n",
      "Epoch 13/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.9452 - acc: 0.6764 - val_loss: 0.8986 - val_acc: 0.6772\n",
      "Epoch 14/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.8690 - acc: 0.6780 - val_loss: 0.8336 - val_acc: 0.6767\n",
      "Epoch 15/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.8118 - acc: 0.6777 - val_loss: 0.7851 - val_acc: 0.6772\n",
      "Epoch 16/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.7694 - acc: 0.6787 - val_loss: 0.7490 - val_acc: 0.6777\n",
      "Epoch 17/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.7375 - acc: 0.6788 - val_loss: 0.7222 - val_acc: 0.6791\n",
      "Epoch 18/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.7142 - acc: 0.6797 - val_loss: 0.7031 - val_acc: 0.6791\n",
      "Epoch 19/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6973 - acc: 0.6792 - val_loss: 0.6886 - val_acc: 0.6791\n",
      "Epoch 20/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6845 - acc: 0.6799 - val_loss: 0.6784 - val_acc: 0.6805\n",
      "Epoch 21/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6752 - acc: 0.6803 - val_loss: 0.6707 - val_acc: 0.6815\n",
      "Epoch 22/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6684 - acc: 0.6809 - val_loss: 0.6652 - val_acc: 0.6805\n",
      "Epoch 23/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6635 - acc: 0.6805 - val_loss: 0.6607 - val_acc: 0.6801\n",
      "Epoch 24/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6600 - acc: 0.6802 - val_loss: 0.6582 - val_acc: 0.6801\n",
      "Epoch 25/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6579 - acc: 0.6806 - val_loss: 0.6556 - val_acc: 0.6796\n",
      "Epoch 26/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6550 - acc: 0.6803 - val_loss: 0.6543 - val_acc: 0.6805\n",
      "Epoch 27/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6535 - acc: 0.6805 - val_loss: 0.6529 - val_acc: 0.6801\n",
      "Epoch 28/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6523 - acc: 0.6806 - val_loss: 0.6521 - val_acc: 0.6805\n",
      "Epoch 29/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6511 - acc: 0.6798 - val_loss: 0.6507 - val_acc: 0.6810\n",
      "Epoch 30/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6501 - acc: 0.6814 - val_loss: 0.6499 - val_acc: 0.6815\n",
      "Epoch 31/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6492 - acc: 0.6806 - val_loss: 0.6490 - val_acc: 0.6819\n",
      "Epoch 32/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6486 - acc: 0.6806 - val_loss: 0.6486 - val_acc: 0.6834\n",
      "Epoch 33/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6479 - acc: 0.6827 - val_loss: 0.6478 - val_acc: 0.6815\n",
      "Epoch 34/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6472 - acc: 0.6836 - val_loss: 0.6477 - val_acc: 0.6824\n",
      "Epoch 35/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6466 - acc: 0.6823 - val_loss: 0.6474 - val_acc: 0.6853\n",
      "Epoch 36/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6461 - acc: 0.6823 - val_loss: 0.6469 - val_acc: 0.6834\n",
      "Epoch 37/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6459 - acc: 0.6812 - val_loss: 0.6463 - val_acc: 0.6843\n",
      "Epoch 38/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6450 - acc: 0.6834 - val_loss: 0.6453 - val_acc: 0.6848\n",
      "Epoch 39/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6445 - acc: 0.6844 - val_loss: 0.6449 - val_acc: 0.6843\n",
      "Epoch 40/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6440 - acc: 0.6833 - val_loss: 0.6444 - val_acc: 0.6848\n",
      "Epoch 41/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6434 - acc: 0.6829 - val_loss: 0.6445 - val_acc: 0.68430.68\n",
      "Epoch 42/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6430 - acc: 0.6847 - val_loss: 0.6441 - val_acc: 0.6848\n",
      "Epoch 43/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6427 - acc: 0.6840 - val_loss: 0.6448 - val_acc: 0.6848\n",
      "Epoch 44/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6425 - acc: 0.6850 - val_loss: 0.6430 - val_acc: 0.6909\n",
      "Epoch 45/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6419 - acc: 0.6848 - val_loss: 0.6422 - val_acc: 0.6909\n",
      "Epoch 46/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6412 - acc: 0.6880 - val_loss: 0.6420 - val_acc: 0.6862\n",
      "Epoch 47/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6412 - acc: 0.6843 - val_loss: 0.6426 - val_acc: 0.6905\n",
      "Epoch 48/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6413 - acc: 0.6848 - val_loss: 0.6416 - val_acc: 0.6862\n",
      "Epoch 49/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6405 - acc: 0.6856 - val_loss: 0.6411 - val_acc: 0.6871\n",
      "Epoch 50/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6398 - acc: 0.6881 - val_loss: 0.6410 - val_acc: 0.6862\n",
      "Epoch 51/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6389 - acc: 0.6876 - val_loss: 0.6406 - val_acc: 0.6952\n",
      "Epoch 52/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6389 - acc: 0.6876 - val_loss: 0.6415 - val_acc: 0.7009\n",
      "Epoch 53/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6385 - acc: 0.6924 - val_loss: 0.6398 - val_acc: 0.6862\n",
      "Epoch 54/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6387 - acc: 0.6867 - val_loss: 0.6399 - val_acc: 0.6961\n",
      "Epoch 55/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6378 - acc: 0.6889 - val_loss: 0.6406 - val_acc: 0.6980\n",
      "Epoch 56/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6372 - acc: 0.6898 - val_loss: 0.6394 - val_acc: 0.6947\n",
      "Epoch 57/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6377 - acc: 0.6907 - val_loss: 0.6390 - val_acc: 0.6848\n",
      "Epoch 58/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6371 - acc: 0.6876 - val_loss: 0.6390 - val_acc: 0.6966\n",
      "Epoch 59/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6362 - acc: 0.6904 - val_loss: 0.6385 - val_acc: 0.6857\n",
      "Epoch 60/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6361 - acc: 0.6892 - val_loss: 0.6394 - val_acc: 0.7032\n",
      "Epoch 61/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6355 - acc: 0.6947 - val_loss: 0.6378 - val_acc: 0.7013\n",
      "Epoch 62/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6366 - acc: 0.6922 - val_loss: 0.6371 - val_acc: 0.6890\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.6347 - acc: 0.6912 - val_loss: 0.6377 - val_acc: 0.6952\n",
      "Epoch 64/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6347 - acc: 0.6901 - val_loss: 0.6376 - val_acc: 0.7032\n",
      "Epoch 65/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6343 - acc: 0.6913 - val_loss: 0.6365 - val_acc: 0.7027\n",
      "Epoch 66/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6343 - acc: 0.6938 - val_loss: 0.6377 - val_acc: 0.7027\n",
      "Epoch 67/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6340 - acc: 0.6967 - val_loss: 0.6357 - val_acc: 0.7013\n",
      "Epoch 68/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6331 - acc: 0.6933 - val_loss: 0.6353 - val_acc: 0.7018\n",
      "Epoch 69/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6329 - acc: 0.6941 - val_loss: 0.6357 - val_acc: 0.7027\n",
      "Epoch 70/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6328 - acc: 0.6952 - val_loss: 0.6346 - val_acc: 0.6966\n",
      "Epoch 71/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6318 - acc: 0.6958 - val_loss: 0.6344 - val_acc: 0.6942\n",
      "Epoch 72/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6318 - acc: 0.6949 - val_loss: 0.6347 - val_acc: 0.6961\n",
      "Epoch 73/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6325 - acc: 0.6924 - val_loss: 0.6334 - val_acc: 0.6961\n",
      "Epoch 74/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6311 - acc: 0.6923 - val_loss: 0.6367 - val_acc: 0.7037\n",
      "Epoch 75/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6313 - acc: 0.6950 - val_loss: 0.6356 - val_acc: 0.7084\n",
      "Epoch 76/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6313 - acc: 0.6966 - val_loss: 0.6326 - val_acc: 0.6994\n",
      "Epoch 77/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6305 - acc: 0.6957 - val_loss: 0.6341 - val_acc: 0.7018\n",
      "Epoch 78/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6313 - acc: 0.6962 - val_loss: 0.6329 - val_acc: 0.6957\n",
      "Epoch 79/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6307 - acc: 0.6956 - val_loss: 0.6330 - val_acc: 0.6994\n",
      "Epoch 80/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6298 - acc: 0.6940 - val_loss: 0.6327 - val_acc: 0.7051\n",
      "Epoch 81/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6296 - acc: 0.6969 - val_loss: 0.6336 - val_acc: 0.6994\n",
      "Epoch 82/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6290 - acc: 0.6958 - val_loss: 0.6320 - val_acc: 0.7042\n",
      "Epoch 83/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6284 - acc: 0.6971 - val_loss: 0.6322 - val_acc: 0.7037\n",
      "Epoch 84/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6285 - acc: 0.6981 - val_loss: 0.6303 - val_acc: 0.7027\n",
      "Epoch 85/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6282 - acc: 0.6972 - val_loss: 0.6316 - val_acc: 0.7042\n",
      "Epoch 86/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6279 - acc: 0.6973 - val_loss: 0.6321 - val_acc: 0.7037\n",
      "Epoch 87/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6274 - acc: 0.6977 - val_loss: 0.6307 - val_acc: 0.70370.703\n",
      "Epoch 88/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6275 - acc: 0.6988 - val_loss: 0.6303 - val_acc: 0.7051\n",
      "Epoch 89/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6272 - acc: 0.6977 - val_loss: 0.6301 - val_acc: 0.7023\n",
      "Epoch 90/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6271 - acc: 0.6976 - val_loss: 0.6313 - val_acc: 0.7027\n",
      "Epoch 91/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6267 - acc: 0.6975 - val_loss: 0.6301 - val_acc: 0.7032\n",
      "Epoch 92/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6261 - acc: 0.6997 - val_loss: 0.6293 - val_acc: 0.7065\n",
      "Epoch 93/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6263 - acc: 0.6985 - val_loss: 0.6296 - val_acc: 0.7051\n",
      "Epoch 94/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6260 - acc: 0.6992 - val_loss: 0.6293 - val_acc: 0.7060\n",
      "Epoch 95/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6263 - acc: 0.7003 - val_loss: 0.6319 - val_acc: 0.7065\n",
      "Epoch 96/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6261 - acc: 0.6995 - val_loss: 0.6301 - val_acc: 0.7065\n",
      "Epoch 97/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6258 - acc: 0.7013 - val_loss: 0.6282 - val_acc: 0.7056\n",
      "Epoch 98/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6249 - acc: 0.6996 - val_loss: 0.6293 - val_acc: 0.7023\n",
      "Epoch 99/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6257 - acc: 0.6994 - val_loss: 0.6295 - val_acc: 0.6952\n",
      "Epoch 100/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6252 - acc: 0.6971 - val_loss: 0.6273 - val_acc: 0.7075\n",
      "Epoch 101/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6242 - acc: 0.7005 - val_loss: 0.6282 - val_acc: 0.6990\n",
      "Epoch 102/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6240 - acc: 0.6983 - val_loss: 0.6282 - val_acc: 0.7037\n",
      "Epoch 103/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6235 - acc: 0.6996 - val_loss: 0.6280 - val_acc: 0.7065\n",
      "Epoch 104/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6233 - acc: 0.7022 - val_loss: 0.6291 - val_acc: 0.7117\n",
      "Epoch 105/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6244 - acc: 0.7000 - val_loss: 0.6260 - val_acc: 0.7089\n",
      "Epoch 106/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6229 - acc: 0.7007 - val_loss: 0.6278 - val_acc: 0.6994\n",
      "Epoch 107/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6238 - acc: 0.7008 - val_loss: 0.6263 - val_acc: 0.7013\n",
      "Epoch 108/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6228 - acc: 0.6981 - val_loss: 0.6278 - val_acc: 0.7004\n",
      "Epoch 109/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6240 - acc: 0.6994 - val_loss: 0.6264 - val_acc: 0.6999\n",
      "Epoch 110/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6240 - acc: 0.6984 - val_loss: 0.6262 - val_acc: 0.7027\n",
      "Epoch 111/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6229 - acc: 0.7009 - val_loss: 0.6268 - val_acc: 0.7079\n",
      "Epoch 112/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6223 - acc: 0.7023 - val_loss: 0.6268 - val_acc: 0.7075\n",
      "Epoch 113/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6216 - acc: 0.7016 - val_loss: 0.6248 - val_acc: 0.7075\n",
      "Epoch 114/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6225 - acc: 0.7012 - val_loss: 0.6260 - val_acc: 0.7060\n",
      "Epoch 115/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6219 - acc: 0.7013 - val_loss: 0.6266 - val_acc: 0.7075\n",
      "Epoch 116/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6217 - acc: 0.7020 - val_loss: 0.6249 - val_acc: 0.7042\n",
      "Epoch 117/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6209 - acc: 0.7023 - val_loss: 0.6244 - val_acc: 0.7023\n",
      "Epoch 118/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6211 - acc: 0.6985 - val_loss: 0.6257 - val_acc: 0.7018\n",
      "Epoch 119/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6225 - acc: 0.6991 - val_loss: 0.6262 - val_acc: 0.7032\n",
      "Epoch 120/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6203 - acc: 0.7005 - val_loss: 0.6267 - val_acc: 0.6975\n",
      "Epoch 121/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6206 - acc: 0.6993 - val_loss: 0.6260 - val_acc: 0.71030.69\n",
      "Epoch 122/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6207 - acc: 0.7020 - val_loss: 0.6260 - val_acc: 0.7108\n",
      "Epoch 123/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6201 - acc: 0.7012 - val_loss: 0.6237 - val_acc: 0.7032\n",
      "Epoch 124/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6197 - acc: 0.7018 - val_loss: 0.6233 - val_acc: 0.7032\n",
      "Epoch 125/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6200 - acc: 0.7004 - val_loss: 0.6241 - val_acc: 0.6999\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.6198 - acc: 0.6996 - val_loss: 0.6229 - val_acc: 0.7070\n",
      "Epoch 127/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6192 - acc: 0.7001 - val_loss: 0.6229 - val_acc: 0.7042\n",
      "Epoch 128/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6190 - acc: 0.7009 - val_loss: 0.6242 - val_acc: 0.7117\n",
      "Epoch 129/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6185 - acc: 0.7018 - val_loss: 0.6232 - val_acc: 0.7108\n",
      "Epoch 130/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6190 - acc: 0.7011 - val_loss: 0.6227 - val_acc: 0.7122\n",
      "Epoch 131/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6190 - acc: 0.7011 - val_loss: 0.6268 - val_acc: 0.7108\n",
      "Epoch 132/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6190 - acc: 0.7010 - val_loss: 0.6261 - val_acc: 0.7122\n",
      "Epoch 133/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6189 - acc: 0.6994 - val_loss: 0.6221 - val_acc: 0.7070\n",
      "Epoch 134/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6179 - acc: 0.7028 - val_loss: 0.6255 - val_acc: 0.7098\n",
      "Epoch 135/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6179 - acc: 0.7014 - val_loss: 0.6229 - val_acc: 0.7084\n",
      "Epoch 136/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6188 - acc: 0.6999 - val_loss: 0.6210 - val_acc: 0.7065\n",
      "Epoch 137/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6185 - acc: 0.7038 - val_loss: 0.6216 - val_acc: 0.7075\n",
      "Epoch 138/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6181 - acc: 0.7004 - val_loss: 0.6230 - val_acc: 0.7065\n",
      "Epoch 139/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6177 - acc: 0.7030 - val_loss: 0.6225 - val_acc: 0.7117\n",
      "Epoch 140/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6169 - acc: 0.7023 - val_loss: 0.6219 - val_acc: 0.7094\n",
      "Epoch 141/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6166 - acc: 0.7028 - val_loss: 0.6220 - val_acc: 0.7084\n",
      "Epoch 142/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6169 - acc: 0.7020 - val_loss: 0.6214 - val_acc: 0.7084\n",
      "Epoch 143/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6167 - acc: 0.7008 - val_loss: 0.6221 - val_acc: 0.7108\n",
      "Epoch 144/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6159 - acc: 0.7029 - val_loss: 0.6211 - val_acc: 0.7079\n",
      "Epoch 145/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6178 - acc: 0.7016 - val_loss: 0.6227 - val_acc: 0.7127\n",
      "Epoch 146/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6180 - acc: 0.7021 - val_loss: 0.6257 - val_acc: 0.7098\n",
      "Epoch 147/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6163 - acc: 0.7025 - val_loss: 0.6190 - val_acc: 0.7112\n",
      "Epoch 148/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6157 - acc: 0.7037 - val_loss: 0.6217 - val_acc: 0.7117\n",
      "Epoch 149/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6165 - acc: 0.7031 - val_loss: 0.6207 - val_acc: 0.7079\n",
      "Epoch 150/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6155 - acc: 0.7038 - val_loss: 0.6213 - val_acc: 0.7117\n",
      "Epoch 151/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6152 - acc: 0.7034 - val_loss: 0.6200 - val_acc: 0.7070\n",
      "Epoch 152/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6148 - acc: 0.7042 - val_loss: 0.6193 - val_acc: 0.7117\n",
      "Epoch 153/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6148 - acc: 0.7035 - val_loss: 0.6220 - val_acc: 0.7131\n",
      "Epoch 154/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6152 - acc: 0.7003 - val_loss: 0.6207 - val_acc: 0.7117\n",
      "Epoch 155/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6161 - acc: 0.7028 - val_loss: 0.6198 - val_acc: 0.7089\n",
      "Epoch 156/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6147 - acc: 0.7032 - val_loss: 0.6191 - val_acc: 0.7108\n",
      "Epoch 157/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6144 - acc: 0.7039 - val_loss: 0.6195 - val_acc: 0.7089\n",
      "Epoch 158/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6140 - acc: 0.7026 - val_loss: 0.6188 - val_acc: 0.7065\n",
      "Epoch 159/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6139 - acc: 0.7025 - val_loss: 0.6186 - val_acc: 0.7089\n",
      "Epoch 160/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6136 - acc: 0.7025 - val_loss: 0.6186 - val_acc: 0.7094\n",
      "Epoch 161/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6140 - acc: 0.7045 - val_loss: 0.6189 - val_acc: 0.7089\n",
      "Epoch 162/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6135 - acc: 0.7035 - val_loss: 0.6181 - val_acc: 0.7084\n",
      "Epoch 163/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6138 - acc: 0.7020 - val_loss: 0.6186 - val_acc: 0.7094\n",
      "Epoch 164/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6129 - acc: 0.7042 - val_loss: 0.6193 - val_acc: 0.7112\n",
      "Epoch 165/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6130 - acc: 0.7044 - val_loss: 0.6179 - val_acc: 0.7103\n",
      "Epoch 166/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6125 - acc: 0.7038 - val_loss: 0.6182 - val_acc: 0.7089\n",
      "Epoch 167/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6126 - acc: 0.7046 - val_loss: 0.6197 - val_acc: 0.7112\n",
      "Epoch 168/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6128 - acc: 0.7056 - val_loss: 0.6180 - val_acc: 0.7084\n",
      "Epoch 169/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6123 - acc: 0.7044 - val_loss: 0.6165 - val_acc: 0.7098\n",
      "Epoch 170/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6126 - acc: 0.7042 - val_loss: 0.6194 - val_acc: 0.7013\n",
      "Epoch 171/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6126 - acc: 0.7034 - val_loss: 0.6178 - val_acc: 0.7065\n",
      "Epoch 172/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6132 - acc: 0.7015 - val_loss: 0.6163 - val_acc: 0.7094\n",
      "Epoch 173/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6137 - acc: 0.7022 - val_loss: 0.6176 - val_acc: 0.7098\n",
      "Epoch 174/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6126 - acc: 0.7048 - val_loss: 0.6175 - val_acc: 0.7117\n",
      "Epoch 175/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6116 - acc: 0.7028 - val_loss: 0.6167 - val_acc: 0.7089\n",
      "Epoch 176/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6114 - acc: 0.7042 - val_loss: 0.6170 - val_acc: 0.7112\n",
      "Epoch 177/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6117 - acc: 0.7018 - val_loss: 0.6158 - val_acc: 0.7112\n",
      "Epoch 178/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6113 - acc: 0.7042 - val_loss: 0.6173 - val_acc: 0.7089\n",
      "Epoch 179/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6111 - acc: 0.7052 - val_loss: 0.6161 - val_acc: 0.7089\n",
      "Epoch 180/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6107 - acc: 0.7038 - val_loss: 0.6163 - val_acc: 0.7094\n",
      "Epoch 181/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6107 - acc: 0.7029 - val_loss: 0.6161 - val_acc: 0.7084\n",
      "Epoch 182/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6104 - acc: 0.7033 - val_loss: 0.6171 - val_acc: 0.7108\n",
      "Epoch 183/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6135 - acc: 0.7013 - val_loss: 0.6183 - val_acc: 0.7084\n",
      "Epoch 184/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6118 - acc: 0.7028 - val_loss: 0.6180 - val_acc: 0.7084\n",
      "Epoch 185/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6114 - acc: 0.7037 - val_loss: 0.6163 - val_acc: 0.7108\n",
      "Epoch 186/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6105 - acc: 0.7043 - val_loss: 0.6161 - val_acc: 0.7094\n",
      "Epoch 187/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6105 - acc: 0.7022 - val_loss: 0.6163 - val_acc: 0.7084\n",
      "Epoch 188/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6100 - acc: 0.7036 - val_loss: 0.6174 - val_acc: 0.7084\n",
      "Epoch 189/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.6098 - acc: 0.7042 - val_loss: 0.6162 - val_acc: 0.7089\n",
      "Epoch 190/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6095 - acc: 0.7038 - val_loss: 0.6156 - val_acc: 0.7098\n",
      "Epoch 191/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6098 - acc: 0.7048 - val_loss: 0.6155 - val_acc: 0.7112\n",
      "Epoch 192/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6100 - acc: 0.7037 - val_loss: 0.6144 - val_acc: 0.7094\n",
      "Epoch 193/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6097 - acc: 0.7043 - val_loss: 0.6146 - val_acc: 0.7094\n",
      "Epoch 194/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6104 - acc: 0.7031 - val_loss: 0.6158 - val_acc: 0.7094\n",
      "Epoch 195/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6132 - acc: 0.6996 - val_loss: 0.6162 - val_acc: 0.7042\n",
      "Epoch 196/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6109 - acc: 0.7012 - val_loss: 0.6163 - val_acc: 0.7070\n",
      "Epoch 197/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6092 - acc: 0.7024 - val_loss: 0.6146 - val_acc: 0.7112\n",
      "Epoch 198/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6089 - acc: 0.7021 - val_loss: 0.6153 - val_acc: 0.7070\n",
      "Epoch 199/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6101 - acc: 0.7031 - val_loss: 0.6141 - val_acc: 0.7094\n",
      "Epoch 200/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6090 - acc: 0.7044 - val_loss: 0.6151 - val_acc: 0.7117\n",
      "Epoch 201/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6090 - acc: 0.7037 - val_loss: 0.6143 - val_acc: 0.7117\n",
      "Epoch 202/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6081 - acc: 0.7043 - val_loss: 0.6148 - val_acc: 0.7098\n",
      "Epoch 203/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6077 - acc: 0.7039 - val_loss: 0.6135 - val_acc: 0.7103\n",
      "Epoch 204/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6084 - acc: 0.7029 - val_loss: 0.6148 - val_acc: 0.7108\n",
      "Epoch 205/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6079 - acc: 0.7040 - val_loss: 0.6136 - val_acc: 0.7098\n",
      "Epoch 206/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6078 - acc: 0.7042 - val_loss: 0.6134 - val_acc: 0.7089\n",
      "Epoch 207/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6075 - acc: 0.7039 - val_loss: 0.6147 - val_acc: 0.7094\n",
      "Epoch 208/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6077 - acc: 0.7041 - val_loss: 0.6137 - val_acc: 0.7103\n",
      "Epoch 209/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6071 - acc: 0.7048 - val_loss: 0.6125 - val_acc: 0.7098\n",
      "Epoch 210/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6078 - acc: 0.7011 - val_loss: 0.6125 - val_acc: 0.7060\n",
      "Epoch 211/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6095 - acc: 0.7008 - val_loss: 0.6130 - val_acc: 0.7103\n",
      "Epoch 212/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6081 - acc: 0.7036 - val_loss: 0.6145 - val_acc: 0.7112\n",
      "Epoch 213/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6069 - acc: 0.7048 - val_loss: 0.6128 - val_acc: 0.7089\n",
      "Epoch 214/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6073 - acc: 0.7045 - val_loss: 0.6125 - val_acc: 0.7094\n",
      "Epoch 215/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6069 - acc: 0.7045 - val_loss: 0.6130 - val_acc: 0.7108\n",
      "Epoch 216/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6069 - acc: 0.7033 - val_loss: 0.6120 - val_acc: 0.7108\n",
      "Epoch 217/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6063 - acc: 0.7044 - val_loss: 0.6137 - val_acc: 0.7108\n",
      "Epoch 218/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6066 - acc: 0.7043 - val_loss: 0.6127 - val_acc: 0.7094\n",
      "Epoch 219/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6075 - acc: 0.7044 - val_loss: 0.6119 - val_acc: 0.7094\n",
      "Epoch 220/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6061 - acc: 0.7035 - val_loss: 0.6119 - val_acc: 0.7103\n",
      "Epoch 221/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6064 - acc: 0.7021 - val_loss: 0.6127 - val_acc: 0.7112\n",
      "Epoch 222/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6065 - acc: 0.7033 - val_loss: 0.6135 - val_acc: 0.7117\n",
      "Epoch 223/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6066 - acc: 0.7036 - val_loss: 0.6124 - val_acc: 0.7112\n",
      "Epoch 224/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6075 - acc: 0.7015 - val_loss: 0.6131 - val_acc: 0.7103\n",
      "Epoch 225/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6065 - acc: 0.7020 - val_loss: 0.6128 - val_acc: 0.7094\n",
      "Epoch 226/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6070 - acc: 0.7032 - val_loss: 0.6128 - val_acc: 0.7084\n",
      "Epoch 227/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6064 - acc: 0.7031 - val_loss: 0.6118 - val_acc: 0.7108\n",
      "Epoch 228/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6057 - acc: 0.7032 - val_loss: 0.6132 - val_acc: 0.7089\n",
      "Epoch 229/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6054 - acc: 0.7056 - val_loss: 0.6121 - val_acc: 0.7103\n",
      "Epoch 230/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6058 - acc: 0.7032 - val_loss: 0.6124 - val_acc: 0.7098\n",
      "Epoch 231/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6060 - acc: 0.7045 - val_loss: 0.6124 - val_acc: 0.7103\n",
      "Epoch 232/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6050 - acc: 0.7035 - val_loss: 0.6111 - val_acc: 0.7098\n",
      "Epoch 233/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6047 - acc: 0.7049 - val_loss: 0.6113 - val_acc: 0.7098\n",
      "Epoch 234/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6049 - acc: 0.7047 - val_loss: 0.6119 - val_acc: 0.7108\n",
      "Epoch 235/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6057 - acc: 0.7041 - val_loss: 0.6105 - val_acc: 0.7094\n",
      "Epoch 236/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6042 - acc: 0.7042 - val_loss: 0.6134 - val_acc: 0.7108\n",
      "Epoch 237/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6052 - acc: 0.7041 - val_loss: 0.6121 - val_acc: 0.7108\n",
      "Epoch 238/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6051 - acc: 0.7043 - val_loss: 0.6108 - val_acc: 0.7098\n",
      "Epoch 239/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6048 - acc: 0.7021 - val_loss: 0.6098 - val_acc: 0.7127\n",
      "Epoch 240/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6050 - acc: 0.7047 - val_loss: 0.6104 - val_acc: 0.7108\n",
      "Epoch 241/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6050 - acc: 0.7032 - val_loss: 0.6103 - val_acc: 0.7122\n",
      "Epoch 242/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6040 - acc: 0.7051 - val_loss: 0.6102 - val_acc: 0.7103\n",
      "Epoch 243/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6038 - acc: 0.7038 - val_loss: 0.6091 - val_acc: 0.7103\n",
      "Epoch 244/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6037 - acc: 0.7036 - val_loss: 0.6128 - val_acc: 0.7112\n",
      "Epoch 245/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6038 - acc: 0.7041 - val_loss: 0.6101 - val_acc: 0.7103\n",
      "Epoch 246/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6034 - acc: 0.7046 - val_loss: 0.6099 - val_acc: 0.7103\n",
      "Epoch 247/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6035 - acc: 0.7025 - val_loss: 0.6092 - val_acc: 0.7103\n",
      "Epoch 248/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6039 - acc: 0.7041 - val_loss: 0.6095 - val_acc: 0.7103\n",
      "Epoch 249/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6036 - acc: 0.7047 - val_loss: 0.6106 - val_acc: 0.7112\n",
      "Epoch 250/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6036 - acc: 0.7040 - val_loss: 0.6088 - val_acc: 0.7108\n",
      "Epoch 251/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6032 - acc: 0.7043 - val_loss: 0.6090 - val_acc: 0.7112\n",
      "Epoch 252/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.6029 - acc: 0.7048 - val_loss: 0.6110 - val_acc: 0.7098\n",
      "Epoch 253/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6036 - acc: 0.7033 - val_loss: 0.6112 - val_acc: 0.7051\n",
      "Epoch 254/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6040 - acc: 0.7036 - val_loss: 0.6097 - val_acc: 0.7065\n",
      "Epoch 255/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6045 - acc: 0.7057 - val_loss: 0.6102 - val_acc: 0.7112\n",
      "Epoch 256/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6034 - acc: 0.7055 - val_loss: 0.6090 - val_acc: 0.7112\n",
      "Epoch 257/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6033 - acc: 0.7037 - val_loss: 0.6084 - val_acc: 0.7112\n",
      "Epoch 258/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6024 - acc: 0.7049 - val_loss: 0.6084 - val_acc: 0.7112\n",
      "Epoch 259/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6027 - acc: 0.7055 - val_loss: 0.6084 - val_acc: 0.7108\n",
      "Epoch 260/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6032 - acc: 0.7034 - val_loss: 0.6081 - val_acc: 0.7098\n",
      "Epoch 261/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6029 - acc: 0.7041 - val_loss: 0.6085 - val_acc: 0.7117\n",
      "Epoch 262/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6027 - acc: 0.7047 - val_loss: 0.6107 - val_acc: 0.7127\n",
      "Epoch 263/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6041 - acc: 0.7011 - val_loss: 0.6093 - val_acc: 0.7098\n",
      "Epoch 264/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6037 - acc: 0.7046 - val_loss: 0.6107 - val_acc: 0.7089\n",
      "Epoch 265/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6034 - acc: 0.7044 - val_loss: 0.6093 - val_acc: 0.7075\n",
      "Epoch 266/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6030 - acc: 0.7040 - val_loss: 0.6098 - val_acc: 0.7127\n",
      "Epoch 267/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6021 - acc: 0.7041 - val_loss: 0.6083 - val_acc: 0.7127\n",
      "Epoch 268/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6022 - acc: 0.7044 - val_loss: 0.6082 - val_acc: 0.7127\n",
      "Epoch 269/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6022 - acc: 0.7057 - val_loss: 0.6080 - val_acc: 0.7112\n",
      "Epoch 270/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6035 - acc: 0.7055 - val_loss: 0.6117 - val_acc: 0.7108\n",
      "Epoch 271/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6028 - acc: 0.7045 - val_loss: 0.6099 - val_acc: 0.7098\n",
      "Epoch 272/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6023 - acc: 0.7033 - val_loss: 0.6072 - val_acc: 0.7131\n",
      "Epoch 273/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6016 - acc: 0.7054 - val_loss: 0.6081 - val_acc: 0.7136\n",
      "Epoch 274/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6009 - acc: 0.7050 - val_loss: 0.6099 - val_acc: 0.7122\n",
      "Epoch 275/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6034 - acc: 0.7033 - val_loss: 0.6100 - val_acc: 0.7094\n",
      "Epoch 276/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6032 - acc: 0.7021 - val_loss: 0.6093 - val_acc: 0.7098\n",
      "Epoch 277/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6013 - acc: 0.7028 - val_loss: 0.6086 - val_acc: 0.7084\n",
      "Epoch 278/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6008 - acc: 0.7033 - val_loss: 0.6076 - val_acc: 0.7089\n",
      "Epoch 279/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6008 - acc: 0.7028 - val_loss: 0.6078 - val_acc: 0.7098\n",
      "Epoch 280/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6018 - acc: 0.7033 - val_loss: 0.6122 - val_acc: 0.7046\n",
      "Epoch 281/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6019 - acc: 0.7019 - val_loss: 0.6081 - val_acc: 0.7112\n",
      "Epoch 282/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6008 - acc: 0.7031 - val_loss: 0.6069 - val_acc: 0.7089\n",
      "Epoch 283/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6012 - acc: 0.7052 - val_loss: 0.6072 - val_acc: 0.7089\n",
      "Epoch 284/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6008 - acc: 0.7039 - val_loss: 0.6072 - val_acc: 0.7117\n",
      "Epoch 285/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6009 - acc: 0.7048 - val_loss: 0.6077 - val_acc: 0.7108\n",
      "Epoch 286/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6006 - acc: 0.7044 - val_loss: 0.6072 - val_acc: 0.7098\n",
      "Epoch 287/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6004 - acc: 0.7048 - val_loss: 0.6072 - val_acc: 0.7117\n",
      "Epoch 288/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6005 - acc: 0.7044 - val_loss: 0.6079 - val_acc: 0.7108\n",
      "Epoch 289/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6001 - acc: 0.7038 - val_loss: 0.6068 - val_acc: 0.7117\n",
      "Epoch 290/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6004 - acc: 0.7044 - val_loss: 0.6078 - val_acc: 0.7108\n",
      "Epoch 291/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6002 - acc: 0.7043 - val_loss: 0.6069 - val_acc: 0.7103\n",
      "Epoch 292/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6001 - acc: 0.7019 - val_loss: 0.6074 - val_acc: 0.7122\n",
      "Epoch 293/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6001 - acc: 0.7039 - val_loss: 0.6054 - val_acc: 0.7103\n",
      "Epoch 294/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6001 - acc: 0.7039 - val_loss: 0.6071 - val_acc: 0.7098\n",
      "Epoch 295/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6006 - acc: 0.7042 - val_loss: 0.6074 - val_acc: 0.7103\n",
      "Epoch 296/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5993 - acc: 0.7048 - val_loss: 0.6067 - val_acc: 0.7117\n",
      "Epoch 297/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6000 - acc: 0.7030 - val_loss: 0.6060 - val_acc: 0.7117\n",
      "Epoch 298/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5996 - acc: 0.7042 - val_loss: 0.6061 - val_acc: 0.7089\n",
      "Epoch 299/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5990 - acc: 0.7041 - val_loss: 0.6063 - val_acc: 0.7108\n",
      "Epoch 300/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5989 - acc: 0.7041 - val_loss: 0.6056 - val_acc: 0.7117\n",
      "Epoch 301/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5987 - acc: 0.7035 - val_loss: 0.6056 - val_acc: 0.7098\n",
      "Epoch 302/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.6003 - acc: 0.7030 - val_loss: 0.6059 - val_acc: 0.7108\n",
      "Epoch 303/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5990 - acc: 0.7049 - val_loss: 0.6066 - val_acc: 0.7098\n",
      "Epoch 304/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5992 - acc: 0.7031 - val_loss: 0.6050 - val_acc: 0.7122\n",
      "Epoch 305/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5994 - acc: 0.7049 - val_loss: 0.6063 - val_acc: 0.7122\n",
      "Epoch 306/500\n",
      "19044/19044 [==============================] - ETA: 0s - loss: 0.5995 - acc: 0.704 - 0s - loss: 0.5994 - acc: 0.7045 - val_loss: 0.6054 - val_acc: 0.7122\n",
      "Epoch 307/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5987 - acc: 0.7052 - val_loss: 0.6056 - val_acc: 0.7112\n",
      "Epoch 308/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5984 - acc: 0.7053 - val_loss: 0.6072 - val_acc: 0.7108\n",
      "Epoch 309/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5984 - acc: 0.7057 - val_loss: 0.6052 - val_acc: 0.7112\n",
      "Epoch 310/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5988 - acc: 0.7056 - val_loss: 0.6056 - val_acc: 0.7108\n",
      "Epoch 311/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5983 - acc: 0.7042 - val_loss: 0.6042 - val_acc: 0.7112\n",
      "Epoch 312/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5987 - acc: 0.7037 - val_loss: 0.6052 - val_acc: 0.7108\n",
      "Epoch 313/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5985 - acc: 0.7039 - val_loss: 0.6057 - val_acc: 0.7117\n",
      "Epoch 314/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.5983 - acc: 0.7038 - val_loss: 0.6059 - val_acc: 0.7112\n",
      "Epoch 315/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5981 - acc: 0.7045 - val_loss: 0.6055 - val_acc: 0.7103\n",
      "Epoch 316/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5977 - acc: 0.7052 - val_loss: 0.6051 - val_acc: 0.7117\n",
      "Epoch 317/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5978 - acc: 0.7044 - val_loss: 0.6055 - val_acc: 0.7112\n",
      "Epoch 318/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5975 - acc: 0.7053 - val_loss: 0.6043 - val_acc: 0.7089\n",
      "Epoch 319/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5978 - acc: 0.7057 - val_loss: 0.6030 - val_acc: 0.7108\n",
      "Epoch 320/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5973 - acc: 0.7060 - val_loss: 0.6051 - val_acc: 0.7108\n",
      "Epoch 321/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5974 - acc: 0.7046 - val_loss: 0.6045 - val_acc: 0.7117\n",
      "Epoch 322/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5978 - acc: 0.7033 - val_loss: 0.6032 - val_acc: 0.7127\n",
      "Epoch 323/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5971 - acc: 0.7055 - val_loss: 0.6039 - val_acc: 0.7117\n",
      "Epoch 324/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5974 - acc: 0.7036 - val_loss: 0.6044 - val_acc: 0.7136\n",
      "Epoch 325/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5974 - acc: 0.7042 - val_loss: 0.6044 - val_acc: 0.7108\n",
      "Epoch 326/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5972 - acc: 0.7048 - val_loss: 0.6038 - val_acc: 0.7112\n",
      "Epoch 327/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5972 - acc: 0.7038 - val_loss: 0.6048 - val_acc: 0.7141\n",
      "Epoch 328/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5984 - acc: 0.7052 - val_loss: 0.6034 - val_acc: 0.7122\n",
      "Epoch 329/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5972 - acc: 0.7058 - val_loss: 0.6039 - val_acc: 0.7117\n",
      "Epoch 330/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5973 - acc: 0.7058 - val_loss: 0.6037 - val_acc: 0.7122\n",
      "Epoch 331/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5968 - acc: 0.7043 - val_loss: 0.6036 - val_acc: 0.7122\n",
      "Epoch 332/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5966 - acc: 0.7031 - val_loss: 0.6021 - val_acc: 0.7108\n",
      "Epoch 333/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5967 - acc: 0.7045 - val_loss: 0.6027 - val_acc: 0.7098\n",
      "Epoch 334/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5973 - acc: 0.7058 - val_loss: 0.6034 - val_acc: 0.7117\n",
      "Epoch 335/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5968 - acc: 0.7056 - val_loss: 0.6051 - val_acc: 0.7127\n",
      "Epoch 336/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5972 - acc: 0.7036 - val_loss: 0.6045 - val_acc: 0.7141\n",
      "Epoch 337/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5976 - acc: 0.7038 - val_loss: 0.6061 - val_acc: 0.7098\n",
      "Epoch 338/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5996 - acc: 0.7014 - val_loss: 0.6048 - val_acc: 0.7103\n",
      "Epoch 339/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5983 - acc: 0.7056 - val_loss: 0.6035 - val_acc: 0.7103\n",
      "Epoch 340/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5962 - acc: 0.7043 - val_loss: 0.6038 - val_acc: 0.7117\n",
      "Epoch 341/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5962 - acc: 0.7055 - val_loss: 0.6027 - val_acc: 0.7108\n",
      "Epoch 342/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5971 - acc: 0.7046 - val_loss: 0.6020 - val_acc: 0.7127\n",
      "Epoch 343/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5968 - acc: 0.7038 - val_loss: 0.6031 - val_acc: 0.7122\n",
      "Epoch 344/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5963 - acc: 0.7040 - val_loss: 0.6019 - val_acc: 0.7103\n",
      "Epoch 345/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5958 - acc: 0.7049 - val_loss: 0.6031 - val_acc: 0.7112\n",
      "Epoch 346/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5957 - acc: 0.7045 - val_loss: 0.6027 - val_acc: 0.7117\n",
      "Epoch 347/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5974 - acc: 0.7055 - val_loss: 0.6039 - val_acc: 0.7079\n",
      "Epoch 348/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5974 - acc: 0.7051 - val_loss: 0.6030 - val_acc: 0.7108\n",
      "Epoch 349/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5970 - acc: 0.7048 - val_loss: 0.6033 - val_acc: 0.7108\n",
      "Epoch 350/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5971 - acc: 0.7055 - val_loss: 0.6055 - val_acc: 0.7098\n",
      "Epoch 351/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5967 - acc: 0.7040 - val_loss: 0.6026 - val_acc: 0.7127\n",
      "Epoch 352/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5955 - acc: 0.7050 - val_loss: 0.6040 - val_acc: 0.7098\n",
      "Epoch 353/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5957 - acc: 0.7042 - val_loss: 0.6023 - val_acc: 0.7108\n",
      "Epoch 354/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5954 - acc: 0.7048 - val_loss: 0.6020 - val_acc: 0.7108\n",
      "Epoch 355/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5959 - acc: 0.7048 - val_loss: 0.6027 - val_acc: 0.7103\n",
      "Epoch 356/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5955 - acc: 0.7034 - val_loss: 0.6017 - val_acc: 0.7103\n",
      "Epoch 357/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5946 - acc: 0.7034 - val_loss: 0.6040 - val_acc: 0.7122\n",
      "Epoch 358/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5967 - acc: 0.7052 - val_loss: 0.6030 - val_acc: 0.7127\n",
      "Epoch 359/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5951 - acc: 0.7051 - val_loss: 0.6032 - val_acc: 0.7098\n",
      "Epoch 360/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5954 - acc: 0.7046 - val_loss: 0.6018 - val_acc: 0.7112\n",
      "Epoch 361/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5960 - acc: 0.7057 - val_loss: 0.6006 - val_acc: 0.7117\n",
      "Epoch 362/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5951 - acc: 0.7057 - val_loss: 0.6034 - val_acc: 0.7094\n",
      "Epoch 363/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5953 - acc: 0.7042 - val_loss: 0.6018 - val_acc: 0.7131\n",
      "Epoch 364/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5949 - acc: 0.7063 - val_loss: 0.6019 - val_acc: 0.7117\n",
      "Epoch 365/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5946 - acc: 0.7047 - val_loss: 0.6005 - val_acc: 0.7141\n",
      "Epoch 366/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5943 - acc: 0.7044 - val_loss: 0.6013 - val_acc: 0.7127\n",
      "Epoch 367/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5945 - acc: 0.7066 - val_loss: 0.6010 - val_acc: 0.7127\n",
      "Epoch 368/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5947 - acc: 0.7057 - val_loss: 0.6028 - val_acc: 0.7117\n",
      "Epoch 369/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5942 - acc: 0.7055 - val_loss: 0.6015 - val_acc: 0.7131\n",
      "Epoch 370/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5950 - acc: 0.7045 - val_loss: 0.6010 - val_acc: 0.7122\n",
      "Epoch 371/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5941 - acc: 0.7056 - val_loss: 0.6027 - val_acc: 0.7089\n",
      "Epoch 372/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5943 - acc: 0.7048 - val_loss: 0.6009 - val_acc: 0.7131\n",
      "Epoch 373/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5940 - acc: 0.7052 - val_loss: 0.6017 - val_acc: 0.7117\n",
      "Epoch 374/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5945 - acc: 0.7052 - val_loss: 0.6006 - val_acc: 0.7108\n",
      "Epoch 375/500\n",
      "19044/19044 [==============================] - ETA: 0s - loss: 0.5925 - acc: 0.707 - 0s - loss: 0.5940 - acc: 0.7049 - val_loss: 0.6007 - val_acc: 0.7117\n",
      "Epoch 376/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.5947 - acc: 0.7053 - val_loss: 0.6015 - val_acc: 0.7117\n",
      "Epoch 377/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5941 - acc: 0.7047 - val_loss: 0.6012 - val_acc: 0.7131\n",
      "Epoch 378/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5950 - acc: 0.7062 - val_loss: 0.6022 - val_acc: 0.7112\n",
      "Epoch 379/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5936 - acc: 0.7058 - val_loss: 0.6001 - val_acc: 0.7141\n",
      "Epoch 380/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5937 - acc: 0.7048 - val_loss: 0.6006 - val_acc: 0.7117\n",
      "Epoch 381/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5934 - acc: 0.7059 - val_loss: 0.6012 - val_acc: 0.7103\n",
      "Epoch 382/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5947 - acc: 0.7048 - val_loss: 0.6013 - val_acc: 0.7117\n",
      "Epoch 383/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5941 - acc: 0.7062 - val_loss: 0.6012 - val_acc: 0.7136\n",
      "Epoch 384/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5938 - acc: 0.7062 - val_loss: 0.6006 - val_acc: 0.7117\n",
      "Epoch 385/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5940 - acc: 0.7039 - val_loss: 0.6000 - val_acc: 0.7136\n",
      "Epoch 386/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5943 - acc: 0.7053 - val_loss: 0.6001 - val_acc: 0.7112\n",
      "Epoch 387/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5942 - acc: 0.7038 - val_loss: 0.6007 - val_acc: 0.7122\n",
      "Epoch 388/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5941 - acc: 0.7050 - val_loss: 0.6019 - val_acc: 0.7150\n",
      "Epoch 389/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5936 - acc: 0.7036 - val_loss: 0.6001 - val_acc: 0.7146\n",
      "Epoch 390/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5939 - acc: 0.7034 - val_loss: 0.6006 - val_acc: 0.7098\n",
      "Epoch 391/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5938 - acc: 0.7048 - val_loss: 0.6018 - val_acc: 0.7117\n",
      "Epoch 392/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5934 - acc: 0.7041 - val_loss: 0.6002 - val_acc: 0.7127\n",
      "Epoch 393/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5931 - acc: 0.7040 - val_loss: 0.6013 - val_acc: 0.7112\n",
      "Epoch 394/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5956 - acc: 0.7005 - val_loss: 0.6027 - val_acc: 0.7103\n",
      "Epoch 395/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5951 - acc: 0.7044 - val_loss: 0.6019 - val_acc: 0.7103\n",
      "Epoch 396/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5937 - acc: 0.7048 - val_loss: 0.6019 - val_acc: 0.7127\n",
      "Epoch 397/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5946 - acc: 0.7030 - val_loss: 0.6002 - val_acc: 0.7098\n",
      "Epoch 398/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5937 - acc: 0.7043 - val_loss: 0.6011 - val_acc: 0.7112\n",
      "Epoch 399/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5938 - acc: 0.7043 - val_loss: 0.6011 - val_acc: 0.7108\n",
      "Epoch 400/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5937 - acc: 0.7041 - val_loss: 0.5992 - val_acc: 0.7122\n",
      "Epoch 401/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5927 - acc: 0.7043 - val_loss: 0.5989 - val_acc: 0.7122\n",
      "Epoch 402/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5927 - acc: 0.7048 - val_loss: 0.5993 - val_acc: 0.7122\n",
      "Epoch 403/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5923 - acc: 0.7040 - val_loss: 0.6002 - val_acc: 0.7108\n",
      "Epoch 404/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5928 - acc: 0.7034 - val_loss: 0.6009 - val_acc: 0.7117\n",
      "Epoch 405/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5926 - acc: 0.7042 - val_loss: 0.5991 - val_acc: 0.7122\n",
      "Epoch 406/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5926 - acc: 0.7039 - val_loss: 0.5999 - val_acc: 0.7117\n",
      "Epoch 407/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5923 - acc: 0.7048 - val_loss: 0.5999 - val_acc: 0.7117\n",
      "Epoch 408/500\n",
      "19044/19044 [==============================] - ETA: 0s - loss: 0.5933 - acc: 0.705 - 0s - loss: 0.5926 - acc: 0.7065 - val_loss: 0.5992 - val_acc: 0.7122\n",
      "Epoch 409/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5925 - acc: 0.7043 - val_loss: 0.5988 - val_acc: 0.7108\n",
      "Epoch 410/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5918 - acc: 0.7052 - val_loss: 0.5992 - val_acc: 0.7117\n",
      "Epoch 411/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5921 - acc: 0.7051 - val_loss: 0.5993 - val_acc: 0.7112\n",
      "Epoch 412/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5917 - acc: 0.7063 - val_loss: 0.5987 - val_acc: 0.7131\n",
      "Epoch 413/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5918 - acc: 0.7057 - val_loss: 0.5981 - val_acc: 0.7112\n",
      "Epoch 414/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5915 - acc: 0.7060 - val_loss: 0.6003 - val_acc: 0.7108\n",
      "Epoch 415/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5930 - acc: 0.7013 - val_loss: 0.6012 - val_acc: 0.7084\n",
      "Epoch 416/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5942 - acc: 0.7024 - val_loss: 0.6047 - val_acc: 0.7079\n",
      "Epoch 417/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5935 - acc: 0.7046 - val_loss: 0.6008 - val_acc: 0.7131\n",
      "Epoch 418/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5923 - acc: 0.7055 - val_loss: 0.5995 - val_acc: 0.7136\n",
      "Epoch 419/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5921 - acc: 0.7038 - val_loss: 0.6008 - val_acc: 0.7089\n",
      "Epoch 420/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5923 - acc: 0.7056 - val_loss: 0.6010 - val_acc: 0.7117\n",
      "Epoch 421/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5940 - acc: 0.7042 - val_loss: 0.6010 - val_acc: 0.7108\n",
      "Epoch 422/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5929 - acc: 0.7055 - val_loss: 0.5994 - val_acc: 0.7122\n",
      "Epoch 423/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5920 - acc: 0.7046 - val_loss: 0.5991 - val_acc: 0.7136\n",
      "Epoch 424/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5917 - acc: 0.7053 - val_loss: 0.5979 - val_acc: 0.7131\n",
      "Epoch 425/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5914 - acc: 0.7047 - val_loss: 0.5980 - val_acc: 0.7150\n",
      "Epoch 426/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5920 - acc: 0.7056 - val_loss: 0.5994 - val_acc: 0.7089\n",
      "Epoch 427/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5921 - acc: 0.7047 - val_loss: 0.5989 - val_acc: 0.7122\n",
      "Epoch 428/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5912 - acc: 0.7046 - val_loss: 0.5993 - val_acc: 0.7089\n",
      "Epoch 429/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5913 - acc: 0.7044 - val_loss: 0.5986 - val_acc: 0.7141\n",
      "Epoch 430/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5911 - acc: 0.7050 - val_loss: 0.5995 - val_acc: 0.7098\n",
      "Epoch 431/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5917 - acc: 0.7035 - val_loss: 0.5989 - val_acc: 0.7127\n",
      "Epoch 432/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5910 - acc: 0.7064 - val_loss: 0.5995 - val_acc: 0.7131\n",
      "Epoch 433/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5911 - acc: 0.7049 - val_loss: 0.5982 - val_acc: 0.7150\n",
      "Epoch 434/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5922 - acc: 0.7042 - val_loss: 0.6021 - val_acc: 0.70600.704\n",
      "Epoch 435/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5932 - acc: 0.7045 - val_loss: 0.5984 - val_acc: 0.7117\n",
      "Epoch 436/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5912 - acc: 0.7055 - val_loss: 0.6009 - val_acc: 0.7117\n",
      "Epoch 437/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5933 - acc: 0.6997 - val_loss: 0.5975 - val_acc: 0.7131\n",
      "Epoch 438/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.5925 - acc: 0.7045 - val_loss: 0.5977 - val_acc: 0.7112\n",
      "Epoch 439/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5916 - acc: 0.7037 - val_loss: 0.6001 - val_acc: 0.70750.705\n",
      "Epoch 440/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5915 - acc: 0.7046 - val_loss: 0.5984 - val_acc: 0.7122\n",
      "Epoch 441/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5916 - acc: 0.7060 - val_loss: 0.5973 - val_acc: 0.7122\n",
      "Epoch 442/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5912 - acc: 0.7042 - val_loss: 0.5972 - val_acc: 0.7117\n",
      "Epoch 443/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5919 - acc: 0.7007 - val_loss: 0.5983 - val_acc: 0.7127\n",
      "Epoch 444/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5914 - acc: 0.7038 - val_loss: 0.5982 - val_acc: 0.7112\n",
      "Epoch 445/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5910 - acc: 0.7044 - val_loss: 0.5978 - val_acc: 0.7112\n",
      "Epoch 446/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5917 - acc: 0.7056 - val_loss: 0.5977 - val_acc: 0.7141\n",
      "Epoch 447/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5912 - acc: 0.7041 - val_loss: 0.5988 - val_acc: 0.7108\n",
      "Epoch 448/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5920 - acc: 0.7040 - val_loss: 0.5980 - val_acc: 0.7141\n",
      "Epoch 449/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5906 - acc: 0.7043 - val_loss: 0.5977 - val_acc: 0.7112\n",
      "Epoch 450/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5913 - acc: 0.7055 - val_loss: 0.5996 - val_acc: 0.7122\n",
      "Epoch 451/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5913 - acc: 0.7051 - val_loss: 0.5973 - val_acc: 0.7131\n",
      "Epoch 452/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5900 - acc: 0.7063 - val_loss: 0.5969 - val_acc: 0.7103\n",
      "Epoch 453/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5902 - acc: 0.7058 - val_loss: 0.5975 - val_acc: 0.7122\n",
      "Epoch 454/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5900 - acc: 0.7061 - val_loss: 0.5969 - val_acc: 0.7131\n",
      "Epoch 455/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5905 - acc: 0.7045 - val_loss: 0.5969 - val_acc: 0.7136\n",
      "Epoch 456/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5906 - acc: 0.7059 - val_loss: 0.5990 - val_acc: 0.7117\n",
      "Epoch 457/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5916 - acc: 0.7063 - val_loss: 0.5977 - val_acc: 0.7112\n",
      "Epoch 458/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5905 - acc: 0.7049 - val_loss: 0.5962 - val_acc: 0.7117\n",
      "Epoch 459/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5931 - acc: 0.7038 - val_loss: 0.6014 - val_acc: 0.7094\n",
      "Epoch 460/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5931 - acc: 0.7028 - val_loss: 0.5951 - val_acc: 0.7141\n",
      "Epoch 461/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5929 - acc: 0.7026 - val_loss: 0.5963 - val_acc: 0.7117\n",
      "Epoch 462/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5907 - acc: 0.7046 - val_loss: 0.5981 - val_acc: 0.7103\n",
      "Epoch 463/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5903 - acc: 0.7049 - val_loss: 0.5965 - val_acc: 0.7117\n",
      "Epoch 464/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5898 - acc: 0.7056 - val_loss: 0.5979 - val_acc: 0.7094\n",
      "Epoch 465/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5902 - acc: 0.7058 - val_loss: 0.5979 - val_acc: 0.7070\n",
      "Epoch 466/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5897 - acc: 0.7040 - val_loss: 0.5970 - val_acc: 0.7127\n",
      "Epoch 467/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5901 - acc: 0.7053 - val_loss: 0.5959 - val_acc: 0.7146\n",
      "Epoch 468/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5901 - acc: 0.7051 - val_loss: 0.5965 - val_acc: 0.7127\n",
      "Epoch 469/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5900 - acc: 0.7052 - val_loss: 0.5975 - val_acc: 0.7117\n",
      "Epoch 470/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5897 - acc: 0.7049 - val_loss: 0.5979 - val_acc: 0.7112\n",
      "Epoch 471/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5899 - acc: 0.7047 - val_loss: 0.5962 - val_acc: 0.7112\n",
      "Epoch 472/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5903 - acc: 0.7050 - val_loss: 0.5971 - val_acc: 0.7117\n",
      "Epoch 473/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5902 - acc: 0.7044 - val_loss: 0.5976 - val_acc: 0.7112\n",
      "Epoch 474/500\n",
      "19044/19044 [==============================] - ETA: 0s - loss: 0.5896 - acc: 0.700 - 0s - loss: 0.5907 - acc: 0.7022 - val_loss: 0.5969 - val_acc: 0.7131\n",
      "Epoch 475/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5909 - acc: 0.7055 - val_loss: 0.5953 - val_acc: 0.7136\n",
      "Epoch 476/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5901 - acc: 0.7053 - val_loss: 0.5968 - val_acc: 0.7127\n",
      "Epoch 477/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5897 - acc: 0.7049 - val_loss: 0.5972 - val_acc: 0.7122\n",
      "Epoch 478/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5893 - acc: 0.7033 - val_loss: 0.5962 - val_acc: 0.7131\n",
      "Epoch 479/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5893 - acc: 0.7057 - val_loss: 0.6003 - val_acc: 0.7056\n",
      "Epoch 480/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5907 - acc: 0.7061 - val_loss: 0.5972 - val_acc: 0.7117\n",
      "Epoch 481/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5897 - acc: 0.7048 - val_loss: 0.5969 - val_acc: 0.7146\n",
      "Epoch 482/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5905 - acc: 0.7050 - val_loss: 0.5967 - val_acc: 0.7131\n",
      "Epoch 483/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5891 - acc: 0.7045 - val_loss: 0.5974 - val_acc: 0.7103\n",
      "Epoch 484/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5891 - acc: 0.7048 - val_loss: 0.5970 - val_acc: 0.7127\n",
      "Epoch 485/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5886 - acc: 0.7058 - val_loss: 0.5962 - val_acc: 0.7122\n",
      "Epoch 486/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5904 - acc: 0.7050 - val_loss: 0.5974 - val_acc: 0.7103\n",
      "Epoch 487/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5892 - acc: 0.7045 - val_loss: 0.5969 - val_acc: 0.7127\n",
      "Epoch 488/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5888 - acc: 0.7051 - val_loss: 0.5973 - val_acc: 0.7098\n",
      "Epoch 489/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5890 - acc: 0.7041 - val_loss: 0.5960 - val_acc: 0.7122\n",
      "Epoch 490/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5898 - acc: 0.7039 - val_loss: 0.5961 - val_acc: 0.7136\n",
      "Epoch 491/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5892 - acc: 0.7039 - val_loss: 0.5967 - val_acc: 0.7131\n",
      "Epoch 492/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5896 - acc: 0.7044 - val_loss: 0.6024 - val_acc: 0.7042\n",
      "Epoch 493/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5898 - acc: 0.7059 - val_loss: 0.5965 - val_acc: 0.7098\n",
      "Epoch 494/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5890 - acc: 0.7050 - val_loss: 0.5963 - val_acc: 0.7122\n",
      "Epoch 495/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5898 - acc: 0.7047 - val_loss: 0.5954 - val_acc: 0.7122\n",
      "Epoch 496/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5887 - acc: 0.7034 - val_loss: 0.5953 - val_acc: 0.7122\n",
      "Epoch 497/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5887 - acc: 0.7050 - val_loss: 0.5974 - val_acc: 0.7117\n",
      "Epoch 498/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5896 - acc: 0.7055 - val_loss: 0.5989 - val_acc: 0.7089\n",
      "Epoch 499/500\n",
      "19044/19044 [==============================] - 0s - loss: 0.5893 - acc: 0.7045 - val_loss: 0.5970 - val_acc: 0.7117\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19044/19044 [==============================] - 0s - loss: 0.5891 - acc: 0.7043 - val_loss: 0.5958 - val_acc: 0.7131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.00, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "oneWeight = (1 - np.mean(yTrain)) / np.mean(yTrain) #force to 5050\n",
    "# zeroWeight = .5 / (1-np.mean(yTrain))\n",
    "class_weight = {0 : 1, 1: oneWeight}\n",
    "print(class_weight)\n",
    "print(\"Have to beat this accuracy\")\n",
    "# print(sklearn.metrics.accuracy_score(yTest, yPred))\n",
    "\n",
    "history = model.fit(xFlatTrain, yTrain,epochs=500, batch_size=1000, verbose=1, \n",
    "           validation_split=0.1, class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'acc', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXP4DRn7bLIlBJdcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2377e908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23277f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4928/5290 [==========================>...] - ETA: 0s[0.59197319225462708, 0.69735349718699613]\n",
      "[[1936  699]\n",
      " [ 902 1753]]\n",
      "accuracy\n",
      "0.697353497164\n",
      "recall\n",
      "0.660263653484\n",
      "precision\n",
      "0.714926590538\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "print(model.evaluate(xFlatTest, yTest,verbose=1))\n",
    "\n",
    "yPred = model.predict(xFlatTest).round()\n",
    "\n",
    "\n",
    "yPred\n",
    "print(confusion_matrix(yTest, yPred))\n",
    "print(\"accuracy\")\n",
    "print(sklearn.metrics.accuracy_score(yTest, yPred))\n",
    "print(\"recall\")\n",
    "print(sklearn.metrics.recall_score(yTest, yPred))\n",
    "print(\"precision\")\n",
    "print(sklearn.metrics.precision_score(yTest, yPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# from h5py import file\n",
    "# import cython\n",
    "# serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"modelBig.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    "\n",
    "model.save('modelSmall.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[433, 989, 986, 856, 656, 108, 426, 571, 308, 141],\n",
       "       [292, 605, 764,  66, 559, 751, 242, 776, 172, 978],\n",
       "       [185,  86, 696, 981, 165, 309, 256, 563, 530,  80],\n",
       "       [864, 685, 816, 333, 945, 524, 333, 701, 178, 593],\n",
       "       [732, 216, 998, 819, 715, 857, 653, 821, 397, 410],\n",
       "       [653, 607, 808, 427, 200, 618, 618, 816, 492, 129],\n",
       "       [293, 129, 414, 492,  10, 775, 750, 109, 191, 815],\n",
       "       [168, 784, 658, 997, 264, 144, 646, 492, 507, 761],\n",
       "       [466, 872, 472, 959, 554,  95, 237, 566, 165, 617],\n",
       "       [334, 425, 773,   0, 898, 882, 926, 616, 811,  69],\n",
       "       [391, 651, 321, 960, 188, 468, 408, 627, 448, 748],\n",
       "       [538,  13, 277, 129, 577, 758, 355, 571, 360, 325],\n",
       "       [ 60, 712, 386, 900,  58, 981,  65, 729, 697, 395],\n",
       "       [338, 988, 140, 612, 672, 795, 705, 694, 589, 760],\n",
       "       [685, 133, 534, 404, 553, 787, 508, 601, 895, 555],\n",
       "       [821, 951, 691, 434, 873, 623, 365, 392, 713, 609],\n",
       "       [220, 240, 147, 700, 622, 749, 468, 895, 454, 803],\n",
       "       [484,  10, 401, 771, 164, 574,  22, 409, 233, 857],\n",
       "       [507,  47, 366, 717, 563, 548, 732,  76, 789, 681],\n",
       "       [942, 899, 283, 661, 189, 335, 117, 620,  15, 927],\n",
       "       [461, 909, 737, 985,  53, 314, 104, 373, 833, 319],\n",
       "       [519, 254, 686, 577, 991, 421, 695, 702, 898, 679],\n",
       "       [457, 505, 803, 609, 964, 970,   0, 868, 934, 543],\n",
       "       [419, 509, 581, 665, 269, 659, 632, 842, 206, 439],\n",
       "       [419, 997, 925, 105, 381, 551, 237, 522, 258, 891],\n",
       "       [600, 764, 913, 816, 221, 288, 413, 224, 387, 868],\n",
       "       [ 26, 788, 235, 185, 961, 910, 497, 484, 978, 495],\n",
       "       [241,  33, 103, 946, 158, 201, 709, 379, 953, 693],\n",
       "       [770, 919, 779, 876, 375, 824, 379, 367, 627, 531],\n",
       "       [331,  85, 817, 176, 663, 241, 286, 146, 742, 733],\n",
       "       [197, 392, 956, 163, 448, 956, 615, 226, 527,  20],\n",
       "       [814, 470, 849, 196, 887,  21, 328, 993, 415,  12]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1000, size=(32, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
